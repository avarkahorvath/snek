{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Library imports, setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you change a file, you dont have to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_metadata, visualize_data, make_dataset\n",
    "from model import build_multitask_model\n",
    "from score_metrics import get_scores\n",
    "from loss import SoftF1Loss #custom loss function, currently not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7956,
     "status": "ok",
     "timestamp": 1760103668680,
     "user": {
      "displayName": "Avarka",
      "userId": "01376155912533068519"
     },
     "user_tz": -120
    },
    "id": "cb7d91df",
    "outputId": "632fa06d-510c-411c-bd31-1a43c8eb8343"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check tf version\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for device in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f\"Found GPU {device.name}, and set memory growth to True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metadata, species_metadata = load_metadata()\n",
    "NUM_SPECIES = len(species_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in data.py\n",
    "visualize_data(image_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "d98e31d7"
   },
   "source": [
    "Loading python images from folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RESOLUTION=224\n",
    "from data import make_batches, split_dataset\n",
    "\n",
    "#split dataset and make batches\n",
    "train_info, val_info, test_info = split_dataset(image_metadata)\n",
    "train_dataset = make_batches(train_info, IMAGE_RESOLUTION)\n",
    "val_dataset   = make_batches(val_info, IMAGE_RESOLUTION)\n",
    "test_dataset  = make_batches(test_info, IMAGE_RESOLUTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_multitask_model(num_species=NUM_SPECIES, image_resolution=IMAGE_RESOLUTION)\n",
    "#print model summary optionally\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model with appropriate losses and metrics for each output\n",
    "\n",
    "lr = 1e-4 #EfficientNetB0 recommends low learning rates\n",
    "\n",
    "#TODO experiment with different optimizers\n",
    "#TODO experiment with different losses\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    \n",
    "    loss={\n",
    "        'species': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'venom': 'binary_crossentropy'\n",
    "        },\n",
    "\n",
    "    #need to balance the losses because species classification is harder than venom classification\n",
    "    loss_weights={\n",
    "        'species': 1.0,\n",
    "        'venom': 0.5\n",
    "       },\n",
    "\n",
    "    #only for monitoring during training\n",
    "    metrics={\n",
    "         'species': 'accuracy',\n",
    "         'venom': 'accuracy'\n",
    "       }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the model only when validation loss improves\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1, #print messages when saving\n",
    ")\n",
    "\n",
    "#training stops if no improvement in validation loss\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "#reduce learning rate when loss has stopped improving\n",
    "reduce_lr_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.3, #multiply lr by this factor\n",
    "    patience=3,\n",
    "    min_lr=1e-6, #minimum lr\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#SPECIES CLASS WEIGHTS\n",
    "# get the different species classes\n",
    "species_classes = np.unique(train_info[\"encoded_id\"])\n",
    "\n",
    "species_cw = compute_class_weight(\n",
    "    class_weight=\"balanced\", #rare classes have larger weights, common classes have smaller weights\n",
    "    classes=species_classes,\n",
    "    y=train_info[\"encoded_id\"]\n",
    ")\n",
    "\n",
    "#build dictionary mapping class to weight\n",
    "species_cw_dict = {int(c): w for c, w in zip(species_classes, species_cw)}\n",
    "\n",
    "species_weight_vec = tf.constant(\n",
    "    [species_cw_dict[i] for i in range(len(species_cw_dict))],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "#VENOM CLASS WEIGHTS\n",
    "#get the different venom classes\n",
    "venom_classes = np.unique(train_info[\"MIVS\"]) \n",
    "\n",
    "venom_cw = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=venom_classes,\n",
    "    y=train_info[\"MIVS\"]\n",
    ")\n",
    "#build dictionary mapping class to weight\n",
    "venom_cw_dict = {int(c): w for c, w in zip(venom_classes, venom_cw)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "class_weight = {\n",
    "    \"species\": species_cw_dict,\n",
    "    \"venom\": venom_cw_dict,\n",
    "}\n",
    "\n",
    "#TODO currently not using any class weights\n",
    "#we should experiment with using sample weights or class weights, or maybe Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=n_epochs,\n",
    "    callbacks=[checkpoint_cb, early_stop_cb, reduce_lr_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.keras')  # load best weights back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, species_loss, venom_loss, species_acc, venom_acc = results\n",
    "\n",
    "print(f\"Test species acc: {species_acc*100:0.2f}%\")\n",
    "print(f\"Test venom acc: {venom_acc*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "def example_results_from_dataset(model, ds, species_names, n_examples=5, venom_threshold=0.5):\n",
    "    \"\"\"\n",
    "    ds must yield: (raw_image, {'species': int, 'venom': int})\n",
    "    \"\"\"\n",
    "    ds = ds.unbatch().shuffle(1000)\n",
    "    samples = list(ds.take(n_examples))\n",
    "\n",
    "    imgs = [x[0] for x in samples]   # RAW kÃ©pek (0â€“255)\n",
    "    lbls = [x[1] for x in samples]\n",
    "\n",
    "    # ðŸ”¹ Modellnek: preprocess_input kell a RAW kÃ©pekre\n",
    "    x_raw = tf.stack([tf.cast(img, tf.float32) for img in imgs], axis=0)\n",
    "    x_for_model = preprocess_input(x_raw)\n",
    "\n",
    "    pred_species_logits, pred_venom_prob = model.predict(x_for_model, verbose=0)\n",
    "\n",
    "    plt.figure(figsize=(3.3 * len(imgs), 3.3))\n",
    "    for i, (img, lbl) in enumerate(zip(imgs, lbls), start=1):\n",
    "        true_species = int(lbl[\"species\"].numpy())\n",
    "        true_venom   = int(lbl[\"venom\"].numpy())\n",
    "\n",
    "        pred_species = int(np.argmax(pred_species_logits[i-1]))\n",
    "        pred_venom   = bool(float(pred_venom_prob[i-1][0]) > venom_threshold)\n",
    "\n",
    "        true_name = species_names[true_species] if 0 <= true_species < len(species_names) else str(true_species)\n",
    "        pred_name = species_names[pred_species] if 0 <= pred_species < len(species_names) else str(pred_species)\n",
    "\n",
    "        plt.subplot(1, len(imgs), i)\n",
    "\n",
    "        img_np = np.clip(img.numpy(), 0, 255).astype(np.uint8)\n",
    "\n",
    "        plt.imshow(img_np)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.title(\n",
    "            f\"True: {true_name} ({'Venom' if true_venom else 'Safe'})\\n\"\n",
    "            f\"Pred: {pred_name} ({'Venom' if pred_venom else 'Safe'})\",\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results_from_dataset(model, test_dataset, species_metadata, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Calculating scoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Function to tell if the species is venomous or not, based on encoded_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_own_metrics= get_scores(model, image_metadata, test_dataset, venom_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Plotting mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_per_class_recall_f1(results):\n",
    "    y_true = results[\"y_species_true\"]\n",
    "    y_pred = results[\"y_species_pred\"]\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    class_ids = sorted([int(c) for c in report.keys() if c.isdigit()])\n",
    "\n",
    "    recalls = [report[str(c)][\"recall\"] for c in class_ids]\n",
    "    f1s     = [report[str(c)][\"f1-score\"] for c in class_ids]\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(class_ids, recalls)\n",
    "    plt.title(\"Per-Class Recall\")\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(class_ids, f1s)\n",
    "    plt.title(\"Per-Class F1-Score\")\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.show()\n",
    "import numpy as np\n",
    "\n",
    "def get_top_confused_pairs(cm, species_names, top_k=20):\n",
    "    \"\"\"\n",
    "    Returns the top most confused class pairs from a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: confusion matrix (shape NxN)\n",
    "        species_names: list mapping class_id -> species name\n",
    "        top_k: how many confused pairs to return\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts with:\n",
    "            true_id, pred_id, true_name, pred_name, count\n",
    "    \"\"\"\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)  # remove correct predictions\n",
    "\n",
    "    confusions = []\n",
    "\n",
    "    # Find all non-zero misclassifications\n",
    "    for true_cls in range(cm_no_diag.shape[0]):\n",
    "        for pred_cls in range(cm_no_diag.shape[1]):\n",
    "            count = cm_no_diag[true_cls, pred_cls]\n",
    "            if count > 0:\n",
    "                confusions.append((true_cls, pred_cls, count))\n",
    "\n",
    "    # Sort by count descending\n",
    "    confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Build output list\n",
    "    results = []\n",
    "    for true_id, pred_id, count in confusions[:top_k]:\n",
    "        results.append({\n",
    "            \"true_id\": true_id,\n",
    "            \"pred_id\": pred_id,\n",
    "            \"true_name\": species_names[true_id],\n",
    "            \"pred_name\": species_names[pred_id],\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(results):\n",
    "    y_true = results[\"y_species_true\"]\n",
    "    y_pred = results[\"y_species_pred\"]\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, cmap='Blues')\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    top_errors = get_top_confused_pairs(cm, species_metadata, top_k=20)\n",
    "\n",
    "    for e in top_errors:\n",
    "        print(f\"True {e['true_id']} ({e['true_name']})  \"\n",
    "            f\"--> Pred {e['pred_id']} ({e['pred_name']})  \"\n",
    "            f\"Count={e['count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_class_recall_f1(results_own_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(results_own_metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
