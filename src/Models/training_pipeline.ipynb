{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Library imports, setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you change a file, you dont have to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 20:40:49.687706: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-30 20:40:49.769753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-30 20:40:50.952427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from data import load_metadata, visualize_data, make_dataset\n",
    "from model import build_multitask_model\n",
    "from score_metrics import get_scores\n",
    "from loss import SoftF1Loss #custom loss function, currently not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7956,
     "status": "ok",
     "timestamp": 1760103668680,
     "user": {
      "displayName": "Avarka",
      "userId": "01376155912533068519"
     },
     "user_tz": -120
    },
    "id": "cb7d91df",
    "outputId": "632fa06d-510c-411c-bd31-1a43c8eb8343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n",
      "Found GPU /physical_device:GPU:0, and set memory growth to True.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check tf version\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for device in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f\"Found GPU {device.name}, and set memory growth to True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Egyetem_adat/kigyo2/snek/src/Models/data.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  image_metadata[\"encoded_id\"] = encoder.fit_transform(image_metadata[\"class_id\"])\n",
      "/mnt/d/Egyetem_adat/kigyo2/snek/src/Models/data.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  image_metadata[\"image_path\"] = image_metadata[\"image_path\"].apply(\n"
     ]
    }
   ],
   "source": [
    "image_metadata, species_metadata = load_metadata()\n",
    "NUM_SPECIES = len(species_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number per species class is 1839\n",
      "Minimum number per species class is 96\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIVS\n",
      "0    53164\n",
      "1    13290\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#in data.py\n",
    "visualize_data(image_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "d98e31d7"
   },
   "source": [
    "Loading python images from folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764531652.856538   23420 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "IMAGE_RESOLUTION=544\n",
    "from data import make_batches, split_dataset\n",
    "\n",
    "train_info, val_info = split_dataset(image_metadata)\n",
    "\n",
    "\n",
    "species_classes = np.unique(train_info[\"encoded_id\"])\n",
    "species_cw = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=species_classes,\n",
    "    y=train_info[\"encoded_id\"],\n",
    ")\n",
    "species_cw_dict = {int(c): w for c, w in zip(species_classes, species_cw)}\n",
    "\n",
    "species_weight_vec = tf.constant(\n",
    "    [species_cw_dict[i] for i in range(len(species_cw_dict))],\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "#split dataset and make batches\n",
    "train_dataset = make_batches(\n",
    "    train_info,\n",
    "    IMAGE_RESOLUTION,\n",
    "    species_weight_vec=species_weight_vec,\n",
    ")\n",
    "\n",
    "val_dataset = make_batches(\n",
    "    val_info,\n",
    "    IMAGE_RESOLUTION,\n",
    "    species_weight_vec=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_multitask_model(num_species=NUM_SPECIES, image_resolution=IMAGE_RESOLUTION)\n",
    "#print model summary optionally\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model with appropriate losses and metrics for each output\n",
    "\n",
    "lr = 1e-4 #EfficientNetB0 recommends low learning rates\n",
    "\n",
    "#TODO experiment with different optimizers\n",
    "#TODO experiment with different losses\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    \n",
    "    loss=[\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'binary_crossentropy'\n",
    "    ],\n",
    "\n",
    "    #need to balance the losses because species classification is harder than venom classification\n",
    "    loss_weights=[1.0, 0.5],\n",
    "\n",
    "\n",
    "    #only for monitoring during training\n",
    "    metrics=['accuracy', 'accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the model only when validation loss improves\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1, #print messages when saving\n",
    ")\n",
    "\n",
    "#training stops if no improvement in validation loss\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "#reduce learning rate when loss has stopped improving\n",
    "reduce_lr_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.3, #multiply lr by this factor\n",
    "    patience=3,\n",
    "    min_lr=1e-6, #minimum lr\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "class_weight = {\n",
    "    \"species\": species_cw_dict,\n",
    "    #\"venom\": venom_cw_dict,\n",
    "}\n",
    "\n",
    "#TODO currently not using any class weights\n",
    "#we should experiment with using sample weights or class weights, or maybe Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 20:41:02.831017: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 217/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:49\u001b[0m 491ms/step - loss: 5.9690 - species_accuracy: 0.0074 - species_loss: 5.7136 - venom_accuracy: 0.7861 - venom_loss: 0.5107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 20:42:50.508518: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 700/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:43\u001b[0m 482ms/step - loss: 5.8393 - species_accuracy: 0.0172 - species_loss: 5.6034 - venom_accuracy: 0.8029 - venom_loss: 0.4717"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - loss: 5.5996 - species_accuracy: 0.0359 - species_loss: 5.3754 - venom_accuracy: 0.8109 - venom_loss: 0.4484\n",
      "Epoch 1: val_loss improved from None to 4.30628, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m920s\u001b[0m 548ms/step - loss: 5.2020 - species_accuracy: 0.0647 - species_loss: 4.9903 - venom_accuracy: 0.8184 - venom_loss: 0.4236 - val_loss: 4.3063 - val_species_accuracy: 0.1725 - val_species_loss: 4.1168 - val_venom_accuracy: 0.8390 - val_venom_loss: 0.3796 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1434/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:49\u001b[0m 479ms/step - loss: 4.3095 - species_accuracy: 0.1377 - species_loss: 4.1057 - venom_accuracy: 0.8255 - venom_loss: 0.4075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:07:41.349165: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1504/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:15\u001b[0m 478ms/step - loss: 4.3040 - species_accuracy: 0.1382 - species_loss: 4.1004 - venom_accuracy: 0.8256 - venom_loss: 0.4072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - loss: 4.2917 - species_accuracy: 0.1392 - species_loss: 4.0884 - venom_accuracy: 0.8258 - venom_loss: 0.4066\n",
      "Epoch 2: val_loss improved from 4.30628 to 3.75068, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 530ms/step - loss: 4.1649 - species_accuracy: 0.1501 - species_loss: 3.9638 - venom_accuracy: 0.8278 - venom_loss: 0.4008 - val_loss: 3.7507 - val_species_accuracy: 0.2414 - val_species_loss: 3.5685 - val_venom_accuracy: 0.8504 - val_venom_loss: 0.3635 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m  71/1662\u001b[0m \u001b[37m\u001b[0m \u001b[1m12:09\u001b[0m 458ms/step - loss: 3.8854 - species_accuracy: 0.1911 - species_loss: 3.6799 - venom_accuracy: 0.8260 - venom_loss: 0.4109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1581/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 457ms/step - loss: 3.8130 - species_accuracy: 0.1921 - species_loss: 3.6156 - venom_accuracy: 0.8278 - venom_loss: 0.3950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:22:57.749306: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 3.8107 - species_accuracy: 0.1922 - species_loss: 3.6133 - venom_accuracy: 0.8279 - venom_loss: 0.3947\n",
      "Epoch 3: val_loss improved from 3.75068 to 3.49310, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 3.7596 - species_accuracy: 0.1959 - species_loss: 3.5648 - venom_accuracy: 0.8300 - venom_loss: 0.3899 - val_loss: 3.4931 - val_species_accuracy: 0.2716 - val_species_loss: 3.3122 - val_venom_accuracy: 0.8459 - val_venom_loss: 0.3604 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m 365/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:51\u001b[0m 456ms/step - loss: 3.5719 - species_accuracy: 0.2367 - species_loss: 3.3733 - venom_accuracy: 0.8281 - venom_loss: 0.3972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:27:51.036505: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1528/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:01\u001b[0m 457ms/step - loss: 3.5503 - species_accuracy: 0.2313 - species_loss: 3.3556 - venom_accuracy: 0.8327 - venom_loss: 0.3895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 3.5478 - species_accuracy: 0.2312 - species_loss: 3.3533 - venom_accuracy: 0.8329 - venom_loss: 0.3890\n",
      "Epoch 4: val_loss improved from 3.49310 to 3.33151, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 512ms/step - loss: 3.5147 - species_accuracy: 0.2296 - species_loss: 3.3224 - venom_accuracy: 0.8350 - venom_loss: 0.3843 - val_loss: 3.3315 - val_species_accuracy: 0.2932 - val_species_loss: 3.1549 - val_venom_accuracy: 0.8497 - val_venom_loss: 0.3530 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m 729/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:32\u001b[0m 485ms/step - loss: 3.3681 - species_accuracy: 0.2487 - species_loss: 3.1806 - venom_accuracy: 0.8374 - venom_loss: 0.3749"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1013/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:17\u001b[0m 490ms/step - loss: 3.3610 - species_accuracy: 0.2499 - species_loss: 3.1740 - venom_accuracy: 0.8381 - venom_loss: 0.3740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:47:31.035539: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - loss: 3.3519 - species_accuracy: 0.2518 - species_loss: 3.1646 - venom_accuracy: 0.8383 - venom_loss: 0.3746\n",
      "Epoch 5: val_loss improved from 3.33151 to 3.23018, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m908s\u001b[0m 546ms/step - loss: 3.3335 - species_accuracy: 0.2561 - species_loss: 3.1459 - venom_accuracy: 0.8386 - venom_loss: 0.3763 - val_loss: 3.2302 - val_species_accuracy: 0.3049 - val_species_loss: 3.0561 - val_venom_accuracy: 0.8534 - val_venom_loss: 0.3466 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m 317/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:56\u001b[0m 488ms/step - loss: 3.2032 - species_accuracy: 0.2680 - species_loss: 3.0088 - venom_accuracy: 0.8321 - venom_loss: 0.3889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:56:58.228826: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 628/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:12\u001b[0m 476ms/step - loss: 3.2151 - species_accuracy: 0.2698 - species_loss: 3.0229 - venom_accuracy: 0.8345 - venom_loss: 0.3845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 3.2104 - species_accuracy: 0.2727 - species_loss: 3.0213 - venom_accuracy: 0.8385 - venom_loss: 0.3782\n",
      "Epoch 6: val_loss improved from 3.23018 to 3.13275, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 519ms/step - loss: 3.1939 - species_accuracy: 0.2772 - species_loss: 3.0074 - venom_accuracy: 0.8402 - venom_loss: 0.3742 - val_loss: 3.1328 - val_species_accuracy: 0.3190 - val_species_loss: 2.9594 - val_venom_accuracy: 0.8548 - val_venom_loss: 0.3461 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m 340/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:58\u001b[0m 453ms/step - loss: 3.0928 - species_accuracy: 0.2896 - species_loss: 2.9008 - venom_accuracy: 0.8324 - venom_loss: 0.3840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 22:11:19.887046: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1424/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:47\u001b[0m 453ms/step - loss: 3.0899 - species_accuracy: 0.2916 - species_loss: 2.9046 - venom_accuracy: 0.8406 - venom_loss: 0.3707"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - loss: 3.0886 - species_accuracy: 0.2921 - species_loss: 2.9033 - venom_accuracy: 0.8407 - venom_loss: 0.3705\n",
      "Epoch 7: val_loss improved from 3.13275 to 3.05432, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 506ms/step - loss: 3.0782 - species_accuracy: 0.2954 - species_loss: 2.8942 - venom_accuracy: 0.8420 - venom_loss: 0.3691 - val_loss: 3.0543 - val_species_accuracy: 0.3300 - val_species_loss: 2.8827 - val_venom_accuracy: 0.8540 - val_venom_loss: 0.3430 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m 643/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:40\u001b[0m 452ms/step - loss: 2.9742 - species_accuracy: 0.3142 - species_loss: 2.7964 - venom_accuracy: 0.8500 - venom_loss: 0.3557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1461/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:30\u001b[0m 452ms/step - loss: 2.9750 - species_accuracy: 0.3133 - species_loss: 2.7939 - venom_accuracy: 0.8460 - venom_loss: 0.3621"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 22:33:48.518492: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - loss: 2.9756 - species_accuracy: 0.3132 - species_loss: 2.7942 - venom_accuracy: 0.8457 - venom_loss: 0.3628\n",
      "Epoch 8: val_loss improved from 3.05432 to 2.97424, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m842s\u001b[0m 506ms/step - loss: 2.9794 - species_accuracy: 0.3123 - species_loss: 2.7957 - venom_accuracy: 0.8435 - venom_loss: 0.3674 - val_loss: 2.9742 - val_species_accuracy: 0.3505 - val_species_loss: 2.8034 - val_venom_accuracy: 0.8561 - val_venom_loss: 0.3416 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m 600/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:01\u001b[0m 453ms/step - loss: 2.8755 - species_accuracy: 0.3271 - species_loss: 2.6933 - venom_accuracy: 0.8431 - venom_loss: 0.3643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 22:42:00.223760: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 963/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:16\u001b[0m 453ms/step - loss: 2.8749 - species_accuracy: 0.3276 - species_loss: 2.6927 - venom_accuracy: 0.8432 - venom_loss: 0.3643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - loss: 2.8811 - species_accuracy: 0.3274 - species_loss: 2.6987 - venom_accuracy: 0.8433 - venom_loss: 0.3647\n",
      "Epoch 9: val_loss improved from 2.97424 to 2.92615, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m881s\u001b[0m 506ms/step - loss: 2.8894 - species_accuracy: 0.3265 - species_loss: 2.7072 - venom_accuracy: 0.8443 - venom_loss: 0.3641 - val_loss: 2.9261 - val_species_accuracy: 0.3571 - val_species_loss: 2.7576 - val_venom_accuracy: 0.8578 - val_venom_loss: 0.3380 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m 570/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:14\u001b[0m 453ms/step - loss: 2.8122 - species_accuracy: 0.3402 - species_loss: 2.6321 - venom_accuracy: 0.8489 - venom_loss: 0.3601"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 22:55:48.125197: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1558/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 465ms/step - loss: 2.8107 - species_accuracy: 0.3392 - species_loss: 2.6304 - venom_accuracy: 0.8471 - venom_loss: 0.3606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 2.8106 - species_accuracy: 0.3391 - species_loss: 2.6303 - venom_accuracy: 0.8470 - venom_loss: 0.3606\n",
      "Epoch 10: val_loss improved from 2.92615 to 2.89718, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 521ms/step - loss: 2.8094 - species_accuracy: 0.3378 - species_loss: 2.6295 - venom_accuracy: 0.8463 - venom_loss: 0.3603 - val_loss: 2.8972 - val_species_accuracy: 0.3630 - val_species_loss: 2.7264 - val_venom_accuracy: 0.8581 - val_venom_loss: 0.3406 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m 416/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:38\u001b[0m 465ms/step - loss: 2.7621 - species_accuracy: 0.3353 - species_loss: 2.5842 - venom_accuracy: 0.8490 - venom_loss: 0.3558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 580/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:20\u001b[0m 462ms/step - loss: 2.7529 - species_accuracy: 0.3387 - species_loss: 2.5751 - venom_accuracy: 0.8488 - venom_loss: 0.3556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 23:10:24.166320: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - loss: 2.7450 - species_accuracy: 0.3452 - species_loss: 2.5665 - venom_accuracy: 0.8468 - venom_loss: 0.3570\n",
      "Epoch 11: val_loss improved from 2.89718 to 2.85265, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m879s\u001b[0m 529ms/step - loss: 2.7451 - species_accuracy: 0.3487 - species_loss: 2.5656 - venom_accuracy: 0.8454 - venom_loss: 0.3586 - val_loss: 2.8526 - val_species_accuracy: 0.3720 - val_species_loss: 2.6829 - val_venom_accuracy: 0.8593 - val_venom_loss: 0.3369 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m1361/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:25\u001b[0m 483ms/step - loss: 2.6856 - species_accuracy: 0.3586 - species_loss: 2.5083 - venom_accuracy: 0.8473 - venom_loss: 0.3546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 23:31:32.662666: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1415/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:59\u001b[0m 483ms/step - loss: 2.6854 - species_accuracy: 0.3588 - species_loss: 2.5081 - venom_accuracy: 0.8473 - venom_loss: 0.3546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 2.6847 - species_accuracy: 0.3592 - species_loss: 2.5073 - venom_accuracy: 0.8474 - venom_loss: 0.3548\n",
      "Epoch 12: val_loss improved from 2.85265 to 2.80641, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m898s\u001b[0m 540ms/step - loss: 2.6791 - species_accuracy: 0.3621 - species_loss: 2.5010 - venom_accuracy: 0.8475 - venom_loss: 0.3563 - val_loss: 2.8064 - val_species_accuracy: 0.3795 - val_species_loss: 2.6381 - val_venom_accuracy: 0.8593 - val_venom_loss: 0.3355 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m 113/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12:34\u001b[0m 487ms/step - loss: 2.5915 - species_accuracy: 0.3715 - species_loss: 2.4172 - venom_accuracy: 0.8487 - venom_loss: 0.3486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 488/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:36\u001b[0m 491ms/step - loss: 2.6104 - species_accuracy: 0.3714 - species_loss: 2.4329 - venom_accuracy: 0.8461 - venom_loss: 0.3550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 23:39:32.962289: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 2.6115 - species_accuracy: 0.3727 - species_loss: 2.4341 - venom_accuracy: 0.8476 - venom_loss: 0.3547\n",
      "Epoch 13: val_loss improved from 2.80641 to 2.77367, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m902s\u001b[0m 543ms/step - loss: 2.6199 - species_accuracy: 0.3709 - species_loss: 2.4429 - venom_accuracy: 0.8489 - venom_loss: 0.3548 - val_loss: 2.7737 - val_species_accuracy: 0.3856 - val_species_loss: 2.6080 - val_venom_accuracy: 0.8640 - val_venom_loss: 0.3311 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m 676/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:56\u001b[0m 483ms/step - loss: 2.5568 - species_accuracy: 0.3856 - species_loss: 2.3822 - venom_accuracy: 0.8495 - venom_loss: 0.3493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1431/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:52\u001b[0m 485ms/step - loss: 2.5586 - species_accuracy: 0.3848 - species_loss: 2.3840 - venom_accuracy: 0.8499 - venom_loss: 0.3492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 00:02:09.616203: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 2.5588 - species_accuracy: 0.3845 - species_loss: 2.3840 - venom_accuracy: 0.8498 - venom_loss: 0.3496\n",
      "Epoch 14: val_loss improved from 2.77367 to 2.73699, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m898s\u001b[0m 540ms/step - loss: 2.5597 - species_accuracy: 0.3822 - species_loss: 2.3835 - venom_accuracy: 0.8490 - venom_loss: 0.3519 - val_loss: 2.7370 - val_species_accuracy: 0.3947 - val_species_loss: 2.5687 - val_venom_accuracy: 0.8590 - val_venom_loss: 0.3364 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m 690/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:24\u001b[0m 458ms/step - loss: 2.5100 - species_accuracy: 0.3971 - species_loss: 2.3369 - venom_accuracy: 0.8522 - venom_loss: 0.3463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 962/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:20\u001b[0m 457ms/step - loss: 2.5111 - species_accuracy: 0.3955 - species_loss: 2.3380 - venom_accuracy: 0.8520 - venom_loss: 0.3462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 00:12:53.037397: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 2.5098 - species_accuracy: 0.3936 - species_loss: 2.3361 - venom_accuracy: 0.8512 - venom_loss: 0.3475\n",
      "Epoch 15: val_loss improved from 2.73699 to 2.72179, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 511ms/step - loss: 2.5128 - species_accuracy: 0.3882 - species_loss: 2.3371 - venom_accuracy: 0.8492 - venom_loss: 0.3518 - val_loss: 2.7218 - val_species_accuracy: 0.3884 - val_species_loss: 2.5553 - val_venom_accuracy: 0.8619 - val_venom_loss: 0.3313 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m 534/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:33\u001b[0m 455ms/step - loss: 2.4555 - species_accuracy: 0.3976 - species_loss: 2.2809 - venom_accuracy: 0.8507 - venom_loss: 0.3492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 00:23:45.896744: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 933/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:32\u001b[0m 456ms/step - loss: 2.4552 - species_accuracy: 0.3978 - species_loss: 2.2798 - venom_accuracy: 0.8499 - venom_loss: 0.3507"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.4551 - species_accuracy: 0.3979 - species_loss: 2.2799 - venom_accuracy: 0.8499 - venom_loss: 0.3504\n",
      "Epoch 16: val_loss improved from 2.72179 to 2.70904, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 509ms/step - loss: 2.4653 - species_accuracy: 0.3967 - species_loss: 2.2902 - venom_accuracy: 0.8500 - venom_loss: 0.3504 - val_loss: 2.7090 - val_species_accuracy: 0.3941 - val_species_loss: 2.5433 - val_venom_accuracy: 0.8638 - val_venom_loss: 0.3307 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m1045/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:42\u001b[0m 457ms/step - loss: 2.4022 - species_accuracy: 0.4066 - species_loss: 2.2295 - venom_accuracy: 0.8526 - venom_loss: 0.3454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1405/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:57\u001b[0m 457ms/step - loss: 2.4057 - species_accuracy: 0.4066 - species_loss: 2.2327 - venom_accuracy: 0.8521 - venom_loss: 0.3460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 00:44:31.564202: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.4073 - species_accuracy: 0.4067 - species_loss: 2.2342 - venom_accuracy: 0.8519 - venom_loss: 0.3462\n",
      "Epoch 17: val_loss improved from 2.70904 to 2.68611, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 2.4179 - species_accuracy: 0.4062 - species_loss: 2.2441 - venom_accuracy: 0.8504 - venom_loss: 0.3476 - val_loss: 2.6861 - val_species_accuracy: 0.4021 - val_species_loss: 2.5192 - val_venom_accuracy: 0.8620 - val_venom_loss: 0.3331 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m1164/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:47\u001b[0m 456ms/step - loss: 2.3514 - species_accuracy: 0.4169 - species_loss: 2.1812 - venom_accuracy: 0.8558 - venom_loss: 0.3403"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1474/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:25\u001b[0m 456ms/step - loss: 2.3551 - species_accuracy: 0.4164 - species_loss: 2.1845 - venom_accuracy: 0.8552 - venom_loss: 0.3412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 00:59:10.984297: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.3572 - species_accuracy: 0.4160 - species_loss: 2.1864 - venom_accuracy: 0.8550 - venom_loss: 0.3416\n",
      "Epoch 18: val_loss improved from 2.68611 to 2.66450, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 510ms/step - loss: 2.3731 - species_accuracy: 0.4132 - species_loss: 2.2004 - venom_accuracy: 0.8536 - venom_loss: 0.3450 - val_loss: 2.6645 - val_species_accuracy: 0.4066 - val_species_loss: 2.5005 - val_venom_accuracy: 0.8631 - val_venom_loss: 0.3280 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m 607/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:01\u001b[0m 457ms/step - loss: 2.3229 - species_accuracy: 0.4230 - species_loss: 2.1513 - venom_accuracy: 0.8523 - venom_loss: 0.3432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 703/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:17\u001b[0m 456ms/step - loss: 2.3232 - species_accuracy: 0.4233 - species_loss: 2.1513 - venom_accuracy: 0.8521 - venom_loss: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:07:27.571626: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.3292 - species_accuracy: 0.4229 - species_loss: 2.1566 - venom_accuracy: 0.8513 - venom_loss: 0.3452\n",
      "Epoch 19: val_loss improved from 2.66450 to 2.63328, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 2.3394 - species_accuracy: 0.4221 - species_loss: 2.1656 - venom_accuracy: 0.8506 - venom_loss: 0.3470 - val_loss: 2.6333 - val_species_accuracy: 0.4129 - val_species_loss: 2.4680 - val_venom_accuracy: 0.8632 - val_venom_loss: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m 615/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:58\u001b[0m 457ms/step - loss: 2.2404 - species_accuracy: 0.4374 - species_loss: 2.0715 - venom_accuracy: 0.8542 - venom_loss: 0.3378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1347/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:23\u001b[0m 456ms/step - loss: 2.2619 - species_accuracy: 0.4346 - species_loss: 2.0905 - venom_accuracy: 0.8528 - venom_loss: 0.3427"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:26:29.205129: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.2675 - species_accuracy: 0.4338 - species_loss: 2.0960 - venom_accuracy: 0.8528 - venom_loss: 0.3431\n",
      "Epoch 20: val_loss improved from 2.63328 to 2.62066, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 2.2913 - species_accuracy: 0.4301 - species_loss: 2.1192 - venom_accuracy: 0.8525 - venom_loss: 0.3445 - val_loss: 2.6207 - val_species_accuracy: 0.4153 - val_species_loss: 2.4575 - val_venom_accuracy: 0.8619 - val_venom_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m 937/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:30\u001b[0m 456ms/step - loss: 2.2607 - species_accuracy: 0.4367 - species_loss: 2.0881 - venom_accuracy: 0.8539 - venom_loss: 0.3452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:37:30.972469: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1145/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:55\u001b[0m 456ms/step - loss: 2.2584 - species_accuracy: 0.4364 - species_loss: 2.0861 - venom_accuracy: 0.8542 - venom_loss: 0.3446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.2573 - species_accuracy: 0.4355 - species_loss: 2.0853 - venom_accuracy: 0.8544 - venom_loss: 0.3441\n",
      "Epoch 21: val_loss improved from 2.62066 to 2.60523, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 2.2521 - species_accuracy: 0.4331 - species_loss: 2.0804 - venom_accuracy: 0.8543 - venom_loss: 0.3435 - val_loss: 2.6052 - val_species_accuracy: 0.4181 - val_species_loss: 2.4414 - val_venom_accuracy: 0.8656 - val_venom_loss: 0.3268 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m 933/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:32\u001b[0m 457ms/step - loss: 2.2029 - species_accuracy: 0.4454 - species_loss: 2.0268 - venom_accuracy: 0.8478 - venom_loss: 0.3522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1269/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:59\u001b[0m 457ms/step - loss: 2.2067 - species_accuracy: 0.4443 - species_loss: 2.0317 - venom_accuracy: 0.8493 - venom_loss: 0.3500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:54:10.225308: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.2101 - species_accuracy: 0.4437 - species_loss: 2.0359 - venom_accuracy: 0.8505 - venom_loss: 0.3483\n",
      "Epoch 22: val_loss improved from 2.60523 to 2.58300, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 510ms/step - loss: 2.2210 - species_accuracy: 0.4412 - species_loss: 2.0498 - venom_accuracy: 0.8542 - venom_loss: 0.3424 - val_loss: 2.5830 - val_species_accuracy: 0.4239 - val_species_loss: 2.4200 - val_venom_accuracy: 0.8653 - val_venom_loss: 0.3249 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m  11/1662\u001b[0m \u001b[37m\u001b[0m \u001b[1m12:14\u001b[0m 445ms/step - loss: 2.3052 - species_accuracy: 0.4157 - species_loss: 2.1335 - venom_accuracy: 0.8598 - venom_loss: 0.3435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:58:43.942341: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 189/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:15\u001b[0m 458ms/step - loss: 2.1287 - species_accuracy: 0.4481 - species_loss: 1.9650 - venom_accuracy: 0.8684 - venom_loss: 0.3274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.1608 - species_accuracy: 0.4480 - species_loss: 1.9922 - venom_accuracy: 0.8585 - venom_loss: 0.3372\n",
      "Epoch 23: val_loss improved from 2.58300 to 2.57617, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 2.1845 - species_accuracy: 0.4463 - species_loss: 2.0141 - venom_accuracy: 0.8547 - venom_loss: 0.3408 - val_loss: 2.5762 - val_species_accuracy: 0.4241 - val_species_loss: 2.4104 - val_venom_accuracy: 0.8638 - val_venom_loss: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m 292/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:24\u001b[0m 456ms/step - loss: 2.1503 - species_accuracy: 0.4629 - species_loss: 1.9783 - venom_accuracy: 0.8525 - venom_loss: 0.3440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 813/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:26\u001b[0m 456ms/step - loss: 2.1434 - species_accuracy: 0.4594 - species_loss: 1.9722 - venom_accuracy: 0.8539 - venom_loss: 0.3424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:18:57.484909: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 2.1415 - species_accuracy: 0.4569 - species_loss: 1.9712 - venom_accuracy: 0.8552 - venom_loss: 0.3406\n",
      "Epoch 24: val_loss improved from 2.57617 to 2.55651, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 509ms/step - loss: 2.1441 - species_accuracy: 0.4538 - species_loss: 1.9744 - venom_accuracy: 0.8562 - venom_loss: 0.3394 - val_loss: 2.5565 - val_species_accuracy: 0.4281 - val_species_loss: 2.3944 - val_venom_accuracy: 0.8670 - val_venom_loss: 0.3226 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m 341/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:04\u001b[0m 457ms/step - loss: 2.0939 - species_accuracy: 0.4584 - species_loss: 1.9254 - venom_accuracy: 0.8525 - venom_loss: 0.3369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1119/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:08\u001b[0m 457ms/step - loss: 2.1026 - species_accuracy: 0.4590 - species_loss: 1.9335 - venom_accuracy: 0.8543 - venom_loss: 0.3381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:35:25.056252: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.1070 - species_accuracy: 0.4591 - species_loss: 1.9378 - venom_accuracy: 0.8546 - venom_loss: 0.3385\n",
      "Epoch 25: val_loss improved from 2.55651 to 2.54356, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 2.1221 - species_accuracy: 0.4587 - species_loss: 1.9521 - venom_accuracy: 0.8544 - venom_loss: 0.3407 - val_loss: 2.5436 - val_species_accuracy: 0.4301 - val_species_loss: 2.3819 - val_venom_accuracy: 0.8677 - val_venom_loss: 0.3219 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m 173/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:15\u001b[0m 453ms/step - loss: 2.0330 - species_accuracy: 0.4776 - species_loss: 1.8776 - venom_accuracy: 0.8612 - venom_loss: 0.3108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:42:20.629800: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 870/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:02\u001b[0m 457ms/step - loss: 2.0583 - species_accuracy: 0.4686 - species_loss: 1.8950 - venom_accuracy: 0.8587 - venom_loss: 0.3266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.0734 - species_accuracy: 0.4665 - species_loss: 1.9079 - venom_accuracy: 0.8573 - venom_loss: 0.3310\n",
      "Epoch 26: val_loss improved from 2.54356 to 2.53420, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 2.0899 - species_accuracy: 0.4632 - species_loss: 1.9212 - venom_accuracy: 0.8550 - venom_loss: 0.3369 - val_loss: 2.5342 - val_species_accuracy: 0.4307 - val_species_loss: 2.3721 - val_venom_accuracy: 0.8671 - val_venom_loss: 0.3226 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m 505/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:47\u001b[0m 456ms/step - loss: 2.0275 - species_accuracy: 0.4781 - species_loss: 1.8622 - venom_accuracy: 0.8572 - venom_loss: 0.3306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:59:00.483290: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1190/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:35\u001b[0m 457ms/step - loss: 2.0448 - species_accuracy: 0.4742 - species_loss: 1.8771 - venom_accuracy: 0.8564 - venom_loss: 0.3354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.0484 - species_accuracy: 0.4731 - species_loss: 1.8807 - venom_accuracy: 0.8569 - venom_loss: 0.3354\n",
      "Epoch 27: val_loss improved from 2.53420 to 2.52860, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 2.0584 - species_accuracy: 0.4706 - species_loss: 1.8910 - venom_accuracy: 0.8576 - venom_loss: 0.3354 - val_loss: 2.5286 - val_species_accuracy: 0.4339 - val_species_loss: 2.3675 - val_venom_accuracy: 0.8673 - val_venom_loss: 0.3193 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m 319/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:12\u001b[0m 456ms/step - loss: 1.9996 - species_accuracy: 0.4737 - species_loss: 1.8297 - venom_accuracy: 0.8557 - venom_loss: 0.3397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 849/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:11\u001b[0m 457ms/step - loss: 2.0111 - species_accuracy: 0.4733 - species_loss: 1.8431 - venom_accuracy: 0.8559 - venom_loss: 0.3360"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:15:46.764981: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 2.0172 - species_accuracy: 0.4733 - species_loss: 1.8492 - venom_accuracy: 0.8562 - venom_loss: 0.3360\n",
      "Epoch 28: val_loss improved from 2.52860 to 2.50988, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 2.0332 - species_accuracy: 0.4736 - species_loss: 1.8650 - venom_accuracy: 0.8561 - venom_loss: 0.3365 - val_loss: 2.5099 - val_species_accuracy: 0.4352 - val_species_loss: 2.3501 - val_venom_accuracy: 0.8689 - val_venom_loss: 0.3192 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m 256/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:44\u001b[0m 458ms/step - loss: 1.9565 - species_accuracy: 0.4940 - species_loss: 1.7858 - venom_accuracy: 0.8641 - venom_loss: 0.3413"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:25:23.609559: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1317/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:37\u001b[0m 456ms/step - loss: 1.9756 - species_accuracy: 0.4860 - species_loss: 1.8083 - venom_accuracy: 0.8598 - venom_loss: 0.3347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.9799 - species_accuracy: 0.4850 - species_loss: 1.8126 - venom_accuracy: 0.8594 - venom_loss: 0.3347\n",
      "Epoch 29: val_loss improved from 2.50988 to 2.49763, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.9940 - species_accuracy: 0.4810 - species_loss: 1.8265 - venom_accuracy: 0.8577 - venom_loss: 0.3346 - val_loss: 2.4976 - val_species_accuracy: 0.4376 - val_species_loss: 2.3369 - val_venom_accuracy: 0.8662 - val_venom_loss: 0.3207 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m  20/1662\u001b[0m \u001b[37m\u001b[0m \u001b[1m12:04\u001b[0m 442ms/step - loss: 1.9512 - species_accuracy: 0.4991 - species_loss: 1.7894 - venom_accuracy: 0.8631 - venom_loss: 0.3235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:37:43.340491: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 506/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:47\u001b[0m 456ms/step - loss: 1.9966 - species_accuracy: 0.4849 - species_loss: 1.8312 - venom_accuracy: 0.8572 - venom_loss: 0.3308"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.9771 - species_accuracy: 0.4855 - species_loss: 1.8110 - venom_accuracy: 0.8567 - venom_loss: 0.3323\n",
      "Epoch 30: val_loss improved from 2.49763 to 2.47841, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 1.9721 - species_accuracy: 0.4842 - species_loss: 1.8047 - venom_accuracy: 0.8560 - venom_loss: 0.3344 - val_loss: 2.4784 - val_species_accuracy: 0.4462 - val_species_loss: 2.3171 - val_venom_accuracy: 0.8656 - val_venom_loss: 0.3218 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m 761/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:50\u001b[0m 456ms/step - loss: 1.9358 - species_accuracy: 0.4986 - species_loss: 1.7673 - venom_accuracy: 0.8548 - venom_loss: 0.3369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1120/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:07\u001b[0m 456ms/step - loss: 1.9370 - species_accuracy: 0.4960 - species_loss: 1.7687 - venom_accuracy: 0.8555 - venom_loss: 0.3366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 04:00:12.701607: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.9416 - species_accuracy: 0.4934 - species_loss: 1.7735 - venom_accuracy: 0.8561 - venom_loss: 0.3361\n",
      "Epoch 31: val_loss did not improve from 2.47841\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 1.9536 - species_accuracy: 0.4877 - species_loss: 1.7861 - venom_accuracy: 0.8574 - venom_loss: 0.3352 - val_loss: 2.4838 - val_species_accuracy: 0.4417 - val_species_loss: 2.3252 - val_venom_accuracy: 0.8671 - val_venom_loss: 0.3206 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m1381/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:08\u001b[0m 456ms/step - loss: 1.9144 - species_accuracy: 0.4981 - species_loss: 1.7492 - venom_accuracy: 0.8586 - venom_loss: 0.3304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1653/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 457ms/step - loss: 1.9157 - species_accuracy: 0.4978 - species_loss: 1.7502 - venom_accuracy: 0.8582 - venom_loss: 0.3310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 04:18:23.566502: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.9158 - species_accuracy: 0.4978 - species_loss: 1.7503 - venom_accuracy: 0.8582 - venom_loss: 0.3310\n",
      "Epoch 32: val_loss improved from 2.47841 to 2.47622, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 1.9240 - species_accuracy: 0.4955 - species_loss: 1.7568 - venom_accuracy: 0.8558 - venom_loss: 0.3343 - val_loss: 2.4762 - val_species_accuracy: 0.4454 - val_species_loss: 2.3197 - val_venom_accuracy: 0.8686 - val_venom_loss: 0.3162 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m 354/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:58\u001b[0m 457ms/step - loss: 1.8785 - species_accuracy: 0.5135 - species_loss: 1.7046 - venom_accuracy: 0.8433 - venom_loss: 0.3479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 04:22:39.367408: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1037/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:45\u001b[0m 456ms/step - loss: 1.8829 - species_accuracy: 0.5087 - species_loss: 1.7137 - venom_accuracy: 0.8520 - venom_loss: 0.3386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.8899 - species_accuracy: 0.5053 - species_loss: 1.7217 - venom_accuracy: 0.8540 - venom_loss: 0.3364\n",
      "Epoch 33: val_loss improved from 2.47622 to 2.46666, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.9019 - species_accuracy: 0.4992 - species_loss: 1.7353 - venom_accuracy: 0.8575 - venom_loss: 0.3323 - val_loss: 2.4667 - val_species_accuracy: 0.4507 - val_species_loss: 2.3051 - val_venom_accuracy: 0.8670 - val_venom_loss: 0.3231 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m 425/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:24\u001b[0m 457ms/step - loss: 1.8799 - species_accuracy: 0.5098 - species_loss: 1.7161 - venom_accuracy: 0.8600 - venom_loss: 0.3276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 497/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:51\u001b[0m 456ms/step - loss: 1.8771 - species_accuracy: 0.5101 - species_loss: 1.7130 - venom_accuracy: 0.8595 - venom_loss: 0.3281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 04:37:51.740292: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.8672 - species_accuracy: 0.5078 - species_loss: 1.7021 - venom_accuracy: 0.8585 - venom_loss: 0.3301\n",
      "Epoch 34: val_loss improved from 2.46666 to 2.45832, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 1.8688 - species_accuracy: 0.5041 - species_loss: 1.7035 - venom_accuracy: 0.8590 - venom_loss: 0.3305 - val_loss: 2.4583 - val_species_accuracy: 0.4464 - val_species_loss: 2.2988 - val_venom_accuracy: 0.8677 - val_venom_loss: 0.3162 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m 859/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:05\u001b[0m 455ms/step - loss: 1.8582 - species_accuracy: 0.5089 - species_loss: 1.6911 - venom_accuracy: 0.8547 - venom_loss: 0.3341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1653/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 456ms/step - loss: 1.8577 - species_accuracy: 0.5084 - species_loss: 1.6912 - venom_accuracy: 0.8558 - venom_loss: 0.3329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:00:47.446515: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.8576 - species_accuracy: 0.5084 - species_loss: 1.6912 - venom_accuracy: 0.8558 - venom_loss: 0.3328\n",
      "Epoch 35: val_loss improved from 2.45832 to 2.44739, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.8562 - species_accuracy: 0.5076 - species_loss: 1.6906 - venom_accuracy: 0.8575 - venom_loss: 0.3311 - val_loss: 2.4474 - val_species_accuracy: 0.4512 - val_species_loss: 2.2873 - val_venom_accuracy: 0.8665 - val_venom_loss: 0.3195 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m 852/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:10\u001b[0m 457ms/step - loss: 1.8004 - species_accuracy: 0.5154 - species_loss: 1.6345 - venom_accuracy: 0.8586 - venom_loss: 0.3319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:08:50.836109: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1502/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:13\u001b[0m 457ms/step - loss: 1.8101 - species_accuracy: 0.5142 - species_loss: 1.6447 - venom_accuracy: 0.8591 - venom_loss: 0.3310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.8118 - species_accuracy: 0.5140 - species_loss: 1.6464 - venom_accuracy: 0.8591 - venom_loss: 0.3307\n",
      "Epoch 36: val_loss did not improve from 2.44739\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 1.8305 - species_accuracy: 0.5121 - species_loss: 1.6662 - venom_accuracy: 0.8595 - venom_loss: 0.3285 - val_loss: 2.4532 - val_species_accuracy: 0.4480 - val_species_loss: 2.2950 - val_venom_accuracy: 0.8668 - val_venom_loss: 0.3176 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m 896/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:50\u001b[0m 457ms/step - loss: 1.7953 - species_accuracy: 0.5152 - species_loss: 1.6288 - venom_accuracy: 0.8544 - venom_loss: 0.3330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:23:17.875091: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1121/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:07\u001b[0m 457ms/step - loss: 1.7943 - species_accuracy: 0.5152 - species_loss: 1.6280 - venom_accuracy: 0.8551 - venom_loss: 0.3327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.7952 - species_accuracy: 0.5154 - species_loss: 1.6292 - venom_accuracy: 0.8561 - venom_loss: 0.3320\n",
      "Epoch 37: val_loss improved from 2.44739 to 2.43783, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.8024 - species_accuracy: 0.5153 - species_loss: 1.6371 - venom_accuracy: 0.8583 - venom_loss: 0.3300 - val_loss: 2.4378 - val_species_accuracy: 0.4534 - val_species_loss: 2.2788 - val_venom_accuracy: 0.8688 - val_venom_loss: 0.3169 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m 210/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:01\u001b[0m 456ms/step - loss: 1.7967 - species_accuracy: 0.5038 - species_loss: 1.6352 - venom_accuracy: 0.8625 - venom_loss: 0.3229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:32:12.359867: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 627/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:50\u001b[0m 455ms/step - loss: 1.7924 - species_accuracy: 0.5110 - species_loss: 1.6304 - venom_accuracy: 0.8619 - venom_loss: 0.3241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 1.7843 - species_accuracy: 0.5169 - species_loss: 1.6212 - venom_accuracy: 0.8611 - venom_loss: 0.3262\n",
      "Epoch 38: val_loss improved from 2.43783 to 2.42111, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 509ms/step - loss: 1.7849 - species_accuracy: 0.5215 - species_loss: 1.6202 - venom_accuracy: 0.8590 - venom_loss: 0.3299 - val_loss: 2.4211 - val_species_accuracy: 0.4570 - val_species_loss: 2.2623 - val_venom_accuracy: 0.8662 - val_venom_loss: 0.3182 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m 313/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:13\u001b[0m 455ms/step - loss: 1.7650 - species_accuracy: 0.5259 - species_loss: 1.5977 - venom_accuracy: 0.8544 - venom_loss: 0.3345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:47:05.831694: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1217/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:23\u001b[0m 457ms/step - loss: 1.7557 - species_accuracy: 0.5253 - species_loss: 1.5907 - venom_accuracy: 0.8573 - venom_loss: 0.3301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.7571 - species_accuracy: 0.5246 - species_loss: 1.5925 - venom_accuracy: 0.8582 - venom_loss: 0.3292\n",
      "Epoch 39: val_loss did not improve from 2.42111\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 1.7636 - species_accuracy: 0.5235 - species_loss: 1.6001 - venom_accuracy: 0.8608 - venom_loss: 0.3272 - val_loss: 2.4338 - val_species_accuracy: 0.4521 - val_species_loss: 2.2766 - val_venom_accuracy: 0.8683 - val_venom_loss: 0.3138 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m 719/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:11\u001b[0m 458ms/step - loss: 1.7026 - species_accuracy: 0.5342 - species_loss: 1.5390 - venom_accuracy: 0.8582 - venom_loss: 0.3271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 06:04:19.373044: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 891/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:52\u001b[0m 457ms/step - loss: 1.7044 - species_accuracy: 0.5340 - species_loss: 1.5413 - venom_accuracy: 0.8588 - venom_loss: 0.3262"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.7167 - species_accuracy: 0.5323 - species_loss: 1.5539 - venom_accuracy: 0.8595 - venom_loss: 0.3257\n",
      "Epoch 40: val_loss improved from 2.42111 to 2.41415, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.7362 - species_accuracy: 0.5302 - species_loss: 1.5727 - venom_accuracy: 0.8592 - venom_loss: 0.3271 - val_loss: 2.4142 - val_species_accuracy: 0.4599 - val_species_loss: 2.2571 - val_venom_accuracy: 0.8676 - val_venom_loss: 0.3133 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m 306/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:17\u001b[0m 455ms/step - loss: 1.7168 - species_accuracy: 0.5280 - species_loss: 1.5504 - venom_accuracy: 0.8541 - venom_loss: 0.3329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 333/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:08\u001b[0m 458ms/step - loss: 1.7157 - species_accuracy: 0.5286 - species_loss: 1.5496 - venom_accuracy: 0.8547 - venom_loss: 0.3322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 06:15:30.379352: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.7170 - species_accuracy: 0.5324 - species_loss: 1.5533 - venom_accuracy: 0.8590 - venom_loss: 0.3274\n",
      "Epoch 41: val_loss did not improve from 2.41415\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m904s\u001b[0m 544ms/step - loss: 1.7204 - species_accuracy: 0.5327 - species_loss: 1.5563 - venom_accuracy: 0.8595 - venom_loss: 0.3276 - val_loss: 2.4266 - val_species_accuracy: 0.4513 - val_species_loss: 2.2678 - val_venom_accuracy: 0.8685 - val_venom_loss: 0.3172 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m 148/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:31\u001b[0m 457ms/step - loss: 1.6263 - species_accuracy: 0.5414 - species_loss: 1.4564 - venom_accuracy: 0.8532 - venom_loss: 0.3397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 565/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:21\u001b[0m 457ms/step - loss: 1.6444 - species_accuracy: 0.5436 - species_loss: 1.4789 - venom_accuracy: 0.8583 - venom_loss: 0.3312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 06:32:20.086328: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.6760 - species_accuracy: 0.5409 - species_loss: 1.5120 - venom_accuracy: 0.8603 - venom_loss: 0.3281\n",
      "Epoch 42: val_loss did not improve from 2.41415\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 510ms/step - loss: 1.6998 - species_accuracy: 0.5379 - species_loss: 1.5369 - venom_accuracy: 0.8603 - venom_loss: 0.3260 - val_loss: 2.4166 - val_species_accuracy: 0.4597 - val_species_loss: 2.2606 - val_venom_accuracy: 0.8679 - val_venom_loss: 0.3131 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m 658/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:39\u001b[0m 457ms/step - loss: 1.6576 - species_accuracy: 0.5429 - species_loss: 1.4968 - venom_accuracy: 0.8644 - venom_loss: 0.3217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 06:47:11.258258: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1131/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:02\u001b[0m 457ms/step - loss: 1.6595 - species_accuracy: 0.5438 - species_loss: 1.4975 - venom_accuracy: 0.8628 - venom_loss: 0.3240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.6625 - species_accuracy: 0.5434 - species_loss: 1.4999 - venom_accuracy: 0.8620 - venom_loss: 0.3252\n",
      "Epoch 43: val_loss improved from 2.41415 to 2.40268, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 511ms/step - loss: 1.6733 - species_accuracy: 0.5416 - species_loss: 1.5101 - venom_accuracy: 0.8604 - venom_loss: 0.3273 - val_loss: 2.4027 - val_species_accuracy: 0.4605 - val_species_loss: 2.2466 - val_venom_accuracy: 0.8679 - val_venom_loss: 0.3142 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m 489/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:56\u001b[0m 457ms/step - loss: 1.6680 - species_accuracy: 0.5473 - species_loss: 1.5117 - venom_accuracy: 0.8687 - venom_loss: 0.3126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 07:00:03.666191: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 648/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:43\u001b[0m 457ms/step - loss: 1.6646 - species_accuracy: 0.5467 - species_loss: 1.5074 - venom_accuracy: 0.8678 - venom_loss: 0.3145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.6585 - species_accuracy: 0.5457 - species_loss: 1.4984 - venom_accuracy: 0.8645 - venom_loss: 0.3204\n",
      "Epoch 44: val_loss improved from 2.40268 to 2.40016, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.6604 - species_accuracy: 0.5429 - species_loss: 1.4977 - venom_accuracy: 0.8612 - venom_loss: 0.3254 - val_loss: 2.4002 - val_species_accuracy: 0.4622 - val_species_loss: 2.2416 - val_venom_accuracy: 0.8670 - val_venom_loss: 0.3156 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m 569/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:19\u001b[0m 457ms/step - loss: 1.6032 - species_accuracy: 0.5511 - species_loss: 1.4435 - venom_accuracy: 0.8628 - venom_loss: 0.3195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 07:14:48.173294: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 728/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:06\u001b[0m 457ms/step - loss: 1.6053 - species_accuracy: 0.5517 - species_loss: 1.4456 - venom_accuracy: 0.8627 - venom_loss: 0.3195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.6213 - species_accuracy: 0.5504 - species_loss: 1.4606 - venom_accuracy: 0.8615 - venom_loss: 0.3214\n",
      "Epoch 45: val_loss improved from 2.40016 to 2.38563, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.6425 - species_accuracy: 0.5484 - species_loss: 1.4797 - venom_accuracy: 0.8592 - venom_loss: 0.3250 - val_loss: 2.3856 - val_species_accuracy: 0.4649 - val_species_loss: 2.2293 - val_venom_accuracy: 0.8695 - val_venom_loss: 0.3113 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m 585/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:11\u001b[0m 456ms/step - loss: 1.6247 - species_accuracy: 0.5476 - species_loss: 1.4575 - venom_accuracy: 0.8595 - venom_loss: 0.3345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1204/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:29\u001b[0m 457ms/step - loss: 1.6176 - species_accuracy: 0.5500 - species_loss: 1.4523 - venom_accuracy: 0.8605 - venom_loss: 0.3307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 07:33:47.122021: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.6176 - species_accuracy: 0.5502 - species_loss: 1.4531 - venom_accuracy: 0.8610 - venom_loss: 0.3289\n",
      "Epoch 46: val_loss did not improve from 2.38563\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m853s\u001b[0m 513ms/step - loss: 1.6188 - species_accuracy: 0.5498 - species_loss: 1.4569 - venom_accuracy: 0.8623 - venom_loss: 0.3244 - val_loss: 2.3970 - val_species_accuracy: 0.4671 - val_species_loss: 2.2400 - val_venom_accuracy: 0.8673 - val_venom_loss: 0.3158 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m 799/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:37\u001b[0m 461ms/step - loss: 1.5768 - species_accuracy: 0.5568 - species_loss: 1.4150 - venom_accuracy: 0.8629 - venom_loss: 0.3235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1380/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:09\u001b[0m 459ms/step - loss: 1.5849 - species_accuracy: 0.5550 - species_loss: 1.4232 - venom_accuracy: 0.8627 - venom_loss: 0.3233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 07:49:23.417949: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step - loss: 1.5888 - species_accuracy: 0.5544 - species_loss: 1.4271 - venom_accuracy: 0.8626 - venom_loss: 0.3234\n",
      "Epoch 47: val_loss improved from 2.38563 to 2.38226, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m853s\u001b[0m 513ms/step - loss: 1.6104 - species_accuracy: 0.5513 - species_loss: 1.4487 - venom_accuracy: 0.8626 - venom_loss: 0.3235 - val_loss: 2.3823 - val_species_accuracy: 0.4658 - val_species_loss: 2.2259 - val_venom_accuracy: 0.8680 - val_venom_loss: 0.3130 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m 427/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:24\u001b[0m 457ms/step - loss: 1.5521 - species_accuracy: 0.5623 - species_loss: 1.3929 - venom_accuracy: 0.8673 - venom_loss: 0.3184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1375/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:11\u001b[0m 457ms/step - loss: 1.5671 - species_accuracy: 0.5603 - species_loss: 1.4076 - venom_accuracy: 0.8658 - venom_loss: 0.3189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 08:03:31.127887: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.5710 - species_accuracy: 0.5593 - species_loss: 1.4114 - venom_accuracy: 0.8654 - venom_loss: 0.3193\n",
      "Epoch 48: val_loss did not improve from 2.38226\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 510ms/step - loss: 1.5937 - species_accuracy: 0.5540 - species_loss: 1.4334 - venom_accuracy: 0.8630 - venom_loss: 0.3214 - val_loss: 2.3830 - val_species_accuracy: 0.4689 - val_species_loss: 2.2276 - val_venom_accuracy: 0.8665 - val_venom_loss: 0.3123 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m 408/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:34\u001b[0m 458ms/step - loss: 1.5212 - species_accuracy: 0.5799 - species_loss: 1.3622 - venom_accuracy: 0.8666 - venom_loss: 0.3180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 08:10:17.036987: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1070/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:30\u001b[0m 457ms/step - loss: 1.5383 - species_accuracy: 0.5737 - species_loss: 1.3790 - venom_accuracy: 0.8649 - venom_loss: 0.3186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.5480 - species_accuracy: 0.5705 - species_loss: 1.3884 - venom_accuracy: 0.8642 - venom_loss: 0.3192\n",
      "Epoch 49: val_loss did not improve from 2.38226\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 1.5678 - species_accuracy: 0.5638 - species_loss: 1.4074 - venom_accuracy: 0.8626 - venom_loss: 0.3208 - val_loss: 2.3887 - val_species_accuracy: 0.4674 - val_species_loss: 2.2307 - val_venom_accuracy: 0.8688 - val_venom_loss: 0.3158 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m 420/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:23\u001b[0m 453ms/step - loss: 1.4957 - species_accuracy: 0.5789 - species_loss: 1.3336 - venom_accuracy: 0.8601 - venom_loss: 0.3243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1440/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:41\u001b[0m 455ms/step - loss: 1.5214 - species_accuracy: 0.5716 - species_loss: 1.3589 - venom_accuracy: 0.8611 - venom_loss: 0.3250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 08:32:14.539597: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 1.5247 - species_accuracy: 0.5705 - species_loss: 1.3624 - venom_accuracy: 0.8614 - venom_loss: 0.3246\n",
      "Epoch 50: val_loss improved from 2.38226 to 2.37770, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.5508 - species_accuracy: 0.5633 - species_loss: 1.3895 - venom_accuracy: 0.8627 - venom_loss: 0.3225 - val_loss: 2.3777 - val_species_accuracy: 0.4737 - val_species_loss: 2.2221 - val_venom_accuracy: 0.8683 - val_venom_loss: 0.3116 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m 541/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:29\u001b[0m 455ms/step - loss: 1.5158 - species_accuracy: 0.5670 - species_loss: 1.3561 - venom_accuracy: 0.8632 - venom_loss: 0.3194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 866/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:03\u001b[0m 457ms/step - loss: 1.5215 - species_accuracy: 0.5677 - species_loss: 1.3618 - venom_accuracy: 0.8634 - venom_loss: 0.3194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 08:42:02.142132: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.5308 - species_accuracy: 0.5675 - species_loss: 1.3709 - venom_accuracy: 0.8632 - venom_loss: 0.3199\n",
      "Epoch 51: val_loss did not improve from 2.37770\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 509ms/step - loss: 1.5446 - species_accuracy: 0.5659 - species_loss: 1.3838 - venom_accuracy: 0.8619 - venom_loss: 0.3225 - val_loss: 2.3813 - val_species_accuracy: 0.4665 - val_species_loss: 2.2247 - val_venom_accuracy: 0.8692 - val_venom_loss: 0.3130 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m  31/1662\u001b[0m \u001b[37m\u001b[0m \u001b[1m13:09\u001b[0m 484ms/step - loss: 1.4446 - species_accuracy: 0.6121 - species_loss: 1.2802 - venom_accuracy: 0.8681 - venom_loss: 0.3287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 08:49:48.517864: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 482/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:00\u001b[0m 458ms/step - loss: 1.4994 - species_accuracy: 0.5735 - species_loss: 1.3431 - venom_accuracy: 0.8666 - venom_loss: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.5066 - species_accuracy: 0.5716 - species_loss: 1.3484 - venom_accuracy: 0.8652 - venom_loss: 0.3163\n",
      "Epoch 52: val_loss improved from 2.37770 to 2.36978, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.5232 - species_accuracy: 0.5679 - species_loss: 1.3636 - venom_accuracy: 0.8647 - venom_loss: 0.3191 - val_loss: 2.3698 - val_species_accuracy: 0.4725 - val_species_loss: 2.2131 - val_venom_accuracy: 0.8664 - val_venom_loss: 0.3123 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m 227/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:54\u001b[0m 456ms/step - loss: 1.5265 - species_accuracy: 0.5714 - species_loss: 1.3727 - venom_accuracy: 0.8688 - venom_loss: 0.3075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 09:05:24.944161: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1048/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:41\u001b[0m 458ms/step - loss: 1.5085 - species_accuracy: 0.5723 - species_loss: 1.3495 - venom_accuracy: 0.8621 - venom_loss: 0.3179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.5074 - species_accuracy: 0.5715 - species_loss: 1.3480 - venom_accuracy: 0.8620 - venom_loss: 0.3189\n",
      "Epoch 53: val_loss improved from 2.36978 to 2.36258, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.5085 - species_accuracy: 0.5705 - species_loss: 1.3486 - venom_accuracy: 0.8621 - venom_loss: 0.3202 - val_loss: 2.3626 - val_species_accuracy: 0.4733 - val_species_loss: 2.2059 - val_venom_accuracy: 0.8683 - val_venom_loss: 0.3129 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m 205/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:05\u001b[0m 457ms/step - loss: 1.4351 - species_accuracy: 0.5881 - species_loss: 1.2728 - venom_accuracy: 0.8629 - venom_loss: 0.3245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 09:20:15.965893: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 653/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:40\u001b[0m 457ms/step - loss: 1.4528 - species_accuracy: 0.5836 - species_loss: 1.2908 - venom_accuracy: 0.8622 - venom_loss: 0.3239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.4687 - species_accuracy: 0.5797 - species_loss: 1.3079 - venom_accuracy: 0.8628 - venom_loss: 0.3218\n",
      "Epoch 54: val_loss did not improve from 2.36258\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m959s\u001b[0m 546ms/step - loss: 1.4856 - species_accuracy: 0.5765 - species_loss: 1.3257 - venom_accuracy: 0.8645 - venom_loss: 0.3194 - val_loss: 2.3738 - val_species_accuracy: 0.4724 - val_species_loss: 2.2174 - val_venom_accuracy: 0.8694 - val_venom_loss: 0.3111 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m 321/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:10\u001b[0m 455ms/step - loss: 1.4228 - species_accuracy: 0.5819 - species_loss: 1.2671 - venom_accuracy: 0.8593 - venom_loss: 0.3112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 422/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:27\u001b[0m 457ms/step - loss: 1.4246 - species_accuracy: 0.5830 - species_loss: 1.2688 - venom_accuracy: 0.8602 - venom_loss: 0.3115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 09:37:02.384295: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.4417 - species_accuracy: 0.5851 - species_loss: 1.2840 - venom_accuracy: 0.8626 - venom_loss: 0.3154\n",
      "Epoch 55: val_loss improved from 2.36258 to 2.35719, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 510ms/step - loss: 1.4687 - species_accuracy: 0.5815 - species_loss: 1.3091 - venom_accuracy: 0.8630 - venom_loss: 0.3194 - val_loss: 2.3572 - val_species_accuracy: 0.4758 - val_species_loss: 2.2027 - val_venom_accuracy: 0.8688 - val_venom_loss: 0.3091 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m 443/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:16\u001b[0m 456ms/step - loss: 1.4447 - species_accuracy: 0.5889 - species_loss: 1.2816 - venom_accuracy: 0.8612 - venom_loss: 0.3263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 09:51:19.752460: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1621/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 457ms/step - loss: 1.4519 - species_accuracy: 0.5851 - species_loss: 1.2902 - venom_accuracy: 0.8608 - venom_loss: 0.3235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.4522 - species_accuracy: 0.5850 - species_loss: 1.2905 - venom_accuracy: 0.8609 - venom_loss: 0.3234\n",
      "Epoch 56: val_loss did not improve from 2.35719\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.4652 - species_accuracy: 0.5814 - species_loss: 1.3051 - venom_accuracy: 0.8624 - venom_loss: 0.3201 - val_loss: 2.3611 - val_species_accuracy: 0.4740 - val_species_loss: 2.2033 - val_venom_accuracy: 0.8688 - val_venom_loss: 0.3144 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m 169/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:18\u001b[0m 454ms/step - loss: 1.4068 - species_accuracy: 0.6069 - species_loss: 1.2416 - venom_accuracy: 0.8593 - venom_loss: 0.3304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 464/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:06\u001b[0m 456ms/step - loss: 1.4115 - species_accuracy: 0.6022 - species_loss: 1.2491 - venom_accuracy: 0.8618 - venom_loss: 0.3249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:05:37.797347: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - loss: 1.4299 - species_accuracy: 0.5933 - species_loss: 1.2692 - venom_accuracy: 0.8637 - venom_loss: 0.3214\n",
      "Epoch 57: val_loss improved from 2.35719 to 2.35134, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 510ms/step - loss: 1.4447 - species_accuracy: 0.5856 - species_loss: 1.2852 - venom_accuracy: 0.8647 - venom_loss: 0.3187 - val_loss: 2.3513 - val_species_accuracy: 0.4793 - val_species_loss: 2.1957 - val_venom_accuracy: 0.8710 - val_venom_loss: 0.3090 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m 208/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:06\u001b[0m 459ms/step - loss: 1.3999 - species_accuracy: 0.5926 - species_loss: 1.2464 - venom_accuracy: 0.8725 - venom_loss: 0.3071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:17:49.536289: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 554/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:32\u001b[0m 463ms/step - loss: 1.4080 - species_accuracy: 0.5908 - species_loss: 1.2508 - venom_accuracy: 0.8677 - venom_loss: 0.3143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - loss: 1.4203 - species_accuracy: 0.5897 - species_loss: 1.2621 - venom_accuracy: 0.8659 - venom_loss: 0.3162\n",
      "Epoch 58: val_loss did not improve from 2.35134\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m894s\u001b[0m 538ms/step - loss: 1.4349 - species_accuracy: 0.5870 - species_loss: 1.2765 - venom_accuracy: 0.8649 - venom_loss: 0.3170 - val_loss: 2.3576 - val_species_accuracy: 0.4761 - val_species_loss: 2.2017 - val_venom_accuracy: 0.8674 - val_venom_loss: 0.3107 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1079/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:43\u001b[0m 486ms/step - loss: 1.3836 - species_accuracy: 0.5927 - species_loss: 1.2248 - venom_accuracy: 0.8632 - venom_loss: 0.3176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:39:52.252550: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1628/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 487ms/step - loss: 1.3925 - species_accuracy: 0.5924 - species_loss: 1.2338 - venom_accuracy: 0.8633 - venom_loss: 0.3174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 1.3930 - species_accuracy: 0.5924 - species_loss: 1.2343 - venom_accuracy: 0.8633 - venom_loss: 0.3174\n",
      "Epoch 59: val_loss did not improve from 2.35134\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m902s\u001b[0m 543ms/step - loss: 1.4151 - species_accuracy: 0.5914 - species_loss: 1.2569 - venom_accuracy: 0.8641 - venom_loss: 0.3165 - val_loss: 2.3541 - val_species_accuracy: 0.4798 - val_species_loss: 2.1966 - val_venom_accuracy: 0.8688 - val_venom_loss: 0.3153 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m1567/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 489ms/step - loss: 1.3940 - species_accuracy: 0.5949 - species_loss: 1.2352 - venom_accuracy: 0.8643 - venom_loss: 0.3176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1639/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 490ms/step - loss: 1.3942 - species_accuracy: 0.5948 - species_loss: 1.2354 - venom_accuracy: 0.8643 - venom_loss: 0.3175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:59:32.612141: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 1.3943 - species_accuracy: 0.5948 - species_loss: 1.2355 - venom_accuracy: 0.8643 - venom_loss: 0.3175\n",
      "Epoch 60: val_loss did not improve from 2.35134\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 549ms/step - loss: 1.3998 - species_accuracy: 0.5924 - species_loss: 1.2424 - venom_accuracy: 0.8654 - venom_loss: 0.3146 - val_loss: 2.3544 - val_species_accuracy: 0.4781 - val_species_loss: 2.1983 - val_venom_accuracy: 0.8670 - val_venom_loss: 0.3112 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m 727/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:35\u001b[0m 487ms/step - loss: 1.3672 - species_accuracy: 0.6020 - species_loss: 1.2117 - venom_accuracy: 0.8677 - venom_loss: 0.3112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1218/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:37\u001b[0m 489ms/step - loss: 1.3608 - species_accuracy: 0.6041 - species_loss: 1.2046 - venom_accuracy: 0.8671 - venom_loss: 0.3124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 11:11:18.905389: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - loss: 1.3586 - species_accuracy: 0.6048 - species_loss: 1.2022 - venom_accuracy: 0.8671 - venom_loss: 0.3129\n",
      "Epoch 61: val_loss improved from 2.35134 to 2.33613, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m905s\u001b[0m 545ms/step - loss: 1.3546 - species_accuracy: 0.6062 - species_loss: 1.1977 - venom_accuracy: 0.8670 - venom_loss: 0.3136 - val_loss: 2.3361 - val_species_accuracy: 0.4853 - val_species_loss: 2.1789 - val_venom_accuracy: 0.8673 - val_venom_loss: 0.3117 - learning_rate: 3.0000e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m 431/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:44\u001b[0m 475ms/step - loss: 1.3272 - species_accuracy: 0.6078 - species_loss: 1.1739 - venom_accuracy: 0.8701 - venom_loss: 0.3065"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 11:19:52.758377: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1640/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 478ms/step - loss: 1.3484 - species_accuracy: 0.6059 - species_loss: 1.1932 - venom_accuracy: 0.8683 - venom_loss: 0.3105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - loss: 1.3485 - species_accuracy: 0.6058 - species_loss: 1.1933 - venom_accuracy: 0.8683 - venom_loss: 0.3105\n",
      "Epoch 62: val_loss improved from 2.33613 to 2.32671, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m891s\u001b[0m 536ms/step - loss: 1.3540 - species_accuracy: 0.6049 - species_loss: 1.1973 - venom_accuracy: 0.8674 - venom_loss: 0.3133 - val_loss: 2.3267 - val_species_accuracy: 0.4859 - val_species_loss: 2.1723 - val_venom_accuracy: 0.8697 - val_venom_loss: 0.3087 - learning_rate: 3.0000e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m 367/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:22\u001b[0m 480ms/step - loss: 1.3208 - species_accuracy: 0.6155 - species_loss: 1.1629 - venom_accuracy: 0.8678 - venom_loss: 0.3157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1594/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 461ms/step - loss: 1.3338 - species_accuracy: 0.6111 - species_loss: 1.1764 - venom_accuracy: 0.8661 - venom_loss: 0.3147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 11:43:33.763705: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step - loss: 1.3339 - species_accuracy: 0.6111 - species_loss: 1.1766 - venom_accuracy: 0.8662 - venom_loss: 0.3147\n",
      "Epoch 63: val_loss did not improve from 2.32671\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 515ms/step - loss: 1.3373 - species_accuracy: 0.6102 - species_loss: 1.1802 - venom_accuracy: 0.8664 - venom_loss: 0.3137 - val_loss: 2.3282 - val_species_accuracy: 0.4831 - val_species_loss: 2.1734 - val_venom_accuracy: 0.8694 - val_venom_loss: 0.3083 - learning_rate: 3.0000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m 833/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:27\u001b[0m 468ms/step - loss: 1.3383 - species_accuracy: 0.6087 - species_loss: 1.1819 - venom_accuracy: 0.8670 - venom_loss: 0.3129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 11:52:05.051876: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 834/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:27\u001b[0m 468ms/step - loss: 1.3383 - species_accuracy: 0.6087 - species_loss: 1.1819 - venom_accuracy: 0.8670 - venom_loss: 0.3129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - loss: 1.3371 - species_accuracy: 0.6084 - species_loss: 1.1807 - venom_accuracy: 0.8671 - venom_loss: 0.3128\n",
      "Epoch 64: val_loss improved from 2.32671 to 2.32411, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 518ms/step - loss: 1.3392 - species_accuracy: 0.6077 - species_loss: 1.1825 - venom_accuracy: 0.8667 - venom_loss: 0.3136 - val_loss: 2.3241 - val_species_accuracy: 0.4837 - val_species_loss: 2.1701 - val_venom_accuracy: 0.8686 - val_venom_loss: 0.3083 - learning_rate: 3.0000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m 643/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:47\u001b[0m 458ms/step - loss: 1.3436 - species_accuracy: 0.6031 - species_loss: 1.1890 - venom_accuracy: 0.8674 - venom_loss: 0.3091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 12:04:51.380912: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1140/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:59\u001b[0m 458ms/step - loss: 1.3374 - species_accuracy: 0.6047 - species_loss: 1.1822 - venom_accuracy: 0.8678 - venom_loss: 0.3105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - loss: 1.3366 - species_accuracy: 0.6053 - species_loss: 1.1812 - venom_accuracy: 0.8680 - venom_loss: 0.3108\n",
      "Epoch 65: val_loss improved from 2.32411 to 2.31845, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 511ms/step - loss: 1.3361 - species_accuracy: 0.6059 - species_loss: 1.1798 - venom_accuracy: 0.8675 - venom_loss: 0.3126 - val_loss: 2.3184 - val_species_accuracy: 0.4877 - val_species_loss: 2.1655 - val_venom_accuracy: 0.8703 - val_venom_loss: 0.3072 - learning_rate: 3.0000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m 695/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:39\u001b[0m 475ms/step - loss: 1.2955 - species_accuracy: 0.6245 - species_loss: 1.1381 - venom_accuracy: 0.8675 - venom_loss: 0.3149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 12:19:36.581723: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 967/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:29\u001b[0m 474ms/step - loss: 1.3014 - species_accuracy: 0.6223 - species_loss: 1.1438 - venom_accuracy: 0.8667 - venom_loss: 0.3150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - loss: 1.3076 - species_accuracy: 0.6196 - species_loss: 1.1505 - venom_accuracy: 0.8663 - venom_loss: 0.3143\n",
      "Epoch 66: val_loss did not improve from 2.31845\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m885s\u001b[0m 532ms/step - loss: 1.3204 - species_accuracy: 0.6151 - species_loss: 1.1643 - venom_accuracy: 0.8669 - venom_loss: 0.3125 - val_loss: 2.3231 - val_species_accuracy: 0.4835 - val_species_loss: 2.1700 - val_venom_accuracy: 0.8716 - val_venom_loss: 0.3092 - learning_rate: 3.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m 259/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:59\u001b[0m 470ms/step - loss: 1.3101 - species_accuracy: 0.6284 - species_loss: 1.1587 - venom_accuracy: 0.8783 - venom_loss: 0.3026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 836/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:32\u001b[0m 475ms/step - loss: 1.3199 - species_accuracy: 0.6208 - species_loss: 1.1654 - venom_accuracy: 0.8714 - venom_loss: 0.3091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 12:35:28.313301: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 1.3189 - species_accuracy: 0.6184 - species_loss: 1.1637 - venom_accuracy: 0.8698 - venom_loss: 0.3105\n",
      "Epoch 67: val_loss did not improve from 2.31845\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 534ms/step - loss: 1.3208 - species_accuracy: 0.6153 - species_loss: 1.1647 - venom_accuracy: 0.8677 - venom_loss: 0.3124 - val_loss: 2.3210 - val_species_accuracy: 0.4885 - val_species_loss: 2.1677 - val_venom_accuracy: 0.8700 - val_venom_loss: 0.3071 - learning_rate: 3.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m 123/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:56\u001b[0m 466ms/step - loss: 1.4324 - species_accuracy: 0.6113 - species_loss: 1.2680 - venom_accuracy: 0.8556 - venom_loss: 0.3288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 12:44:36.304786: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1353/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:29\u001b[0m 483ms/step - loss: 1.3366 - species_accuracy: 0.6126 - species_loss: 1.1775 - venom_accuracy: 0.8620 - venom_loss: 0.3181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 1.3338 - species_accuracy: 0.6122 - species_loss: 1.1751 - venom_accuracy: 0.8624 - venom_loss: 0.3175\n",
      "Epoch 68: val_loss did not improve from 2.31845\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m884s\u001b[0m 532ms/step - loss: 1.3207 - species_accuracy: 0.6107 - species_loss: 1.1642 - venom_accuracy: 0.8654 - venom_loss: 0.3130 - val_loss: 2.3241 - val_species_accuracy: 0.4877 - val_species_loss: 2.1694 - val_venom_accuracy: 0.8697 - val_venom_loss: 0.3105 - learning_rate: 3.0000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m 634/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:46\u001b[0m 454ms/step - loss: 1.2984 - species_accuracy: 0.6201 - species_loss: 1.1409 - venom_accuracy: 0.8656 - venom_loss: 0.3152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 905/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:44\u001b[0m 455ms/step - loss: 1.2948 - species_accuracy: 0.6212 - species_loss: 1.1375 - venom_accuracy: 0.8663 - venom_loss: 0.3145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 13:05:14.296166: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - loss: 1.2941 - species_accuracy: 0.6212 - species_loss: 1.1372 - venom_accuracy: 0.8670 - venom_loss: 0.3138\n",
      "Epoch 69: val_loss improved from 2.31845 to 2.31635, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 508ms/step - loss: 1.3010 - species_accuracy: 0.6185 - species_loss: 1.1445 - venom_accuracy: 0.8673 - venom_loss: 0.3132 - val_loss: 2.3164 - val_species_accuracy: 0.4880 - val_species_loss: 2.1626 - val_venom_accuracy: 0.8701 - val_venom_loss: 0.3084 - learning_rate: 9.0000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m1471/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:26\u001b[0m 454ms/step - loss: 1.3031 - species_accuracy: 0.6177 - species_loss: 1.1468 - venom_accuracy: 0.8670 - venom_loss: 0.3126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 13:23:35.079143: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1622/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 454ms/step - loss: 1.3031 - species_accuracy: 0.6179 - species_loss: 1.1468 - venom_accuracy: 0.8669 - venom_loss: 0.3126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 1.3031 - species_accuracy: 0.6179 - species_loss: 1.1468 - venom_accuracy: 0.8669 - venom_loss: 0.3125\n",
      "Epoch 70: val_loss improved from 2.31635 to 2.31630, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 508ms/step - loss: 1.3009 - species_accuracy: 0.6194 - species_loss: 1.1456 - venom_accuracy: 0.8669 - venom_loss: 0.3108 - val_loss: 2.3163 - val_species_accuracy: 0.4870 - val_species_loss: 2.1613 - val_venom_accuracy: 0.8710 - val_venom_loss: 0.3089 - learning_rate: 9.0000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m  14/1662\u001b[0m \u001b[37m\u001b[0m \u001b[1m11:51\u001b[0m 432ms/step - loss: 1.1966 - species_accuracy: 0.7048 - species_loss: 1.0605 - venom_accuracy: 0.9002 - venom_loss: 0.2724"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1296/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:51\u001b[0m 469ms/step - loss: 1.2946 - species_accuracy: 0.6226 - species_loss: 1.1391 - venom_accuracy: 0.8688 - venom_loss: 0.3111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 13:36:38.562312: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - loss: 1.2953 - species_accuracy: 0.6218 - species_loss: 1.1395 - venom_accuracy: 0.8684 - venom_loss: 0.3116\n",
      "Epoch 71: val_loss improved from 2.31630 to 2.31614, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m870s\u001b[0m 523ms/step - loss: 1.2993 - species_accuracy: 0.6191 - species_loss: 1.1429 - venom_accuracy: 0.8678 - venom_loss: 0.3130 - val_loss: 2.3161 - val_species_accuracy: 0.4882 - val_species_loss: 2.1621 - val_venom_accuracy: 0.8704 - val_venom_loss: 0.3077 - learning_rate: 9.0000e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m 421/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:23\u001b[0m 454ms/step - loss: 1.2767 - species_accuracy: 0.6244 - species_loss: 1.1238 - venom_accuracy: 0.8679 - venom_loss: 0.3058"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 13:44:11.042903: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 517/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8:43\u001b[0m 457ms/step - loss: 1.2764 - species_accuracy: 0.6240 - species_loss: 1.1231 - venom_accuracy: 0.8676 - venom_loss: 0.3067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.2827 - species_accuracy: 0.6214 - species_loss: 1.1275 - venom_accuracy: 0.8657 - venom_loss: 0.3106\n",
      "Epoch 72: val_loss improved from 2.31614 to 2.31537, saving model to best_model.keras\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 511ms/step - loss: 1.2886 - species_accuracy: 0.6201 - species_loss: 1.1325 - venom_accuracy: 0.8648 - venom_loss: 0.3123 - val_loss: 2.3154 - val_species_accuracy: 0.4877 - val_species_loss: 2.1623 - val_venom_accuracy: 0.8707 - val_venom_loss: 0.3083 - learning_rate: 9.0000e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m 615/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7:56\u001b[0m 456ms/step - loss: 1.2957 - species_accuracy: 0.6120 - species_loss: 1.1399 - venom_accuracy: 0.8689 - venom_loss: 0.3116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 13:59:50.240320: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1080/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:25\u001b[0m 456ms/step - loss: 1.2977 - species_accuracy: 0.6129 - species_loss: 1.1414 - venom_accuracy: 0.8674 - venom_loss: 0.3127"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.2986 - species_accuracy: 0.6142 - species_loss: 1.1421 - venom_accuracy: 0.8665 - venom_loss: 0.3129\n",
      "Epoch 73: val_loss did not improve from 2.31537\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 1.2961 - species_accuracy: 0.6178 - species_loss: 1.1400 - venom_accuracy: 0.8658 - venom_loss: 0.3121 - val_loss: 2.3167 - val_species_accuracy: 0.4864 - val_species_loss: 2.1633 - val_venom_accuracy: 0.8703 - val_venom_loss: 0.3073 - learning_rate: 9.0000e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m1443/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:40\u001b[0m 457ms/step - loss: 1.2873 - species_accuracy: 0.6160 - species_loss: 1.1307 - venom_accuracy: 0.8658 - venom_loss: 0.3131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 14:20:18.251212: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1482/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:22\u001b[0m 457ms/step - loss: 1.2874 - species_accuracy: 0.6161 - species_loss: 1.1309 - venom_accuracy: 0.8658 - venom_loss: 0.3131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.2879 - species_accuracy: 0.6165 - species_loss: 1.1314 - venom_accuracy: 0.8659 - venom_loss: 0.3129\n",
      "Epoch 74: val_loss did not improve from 2.31537\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 511ms/step - loss: 1.2917 - species_accuracy: 0.6200 - species_loss: 1.1358 - venom_accuracy: 0.8663 - venom_loss: 0.3120 - val_loss: 2.3174 - val_species_accuracy: 0.4856 - val_species_loss: 2.1626 - val_venom_accuracy: 0.8703 - val_venom_loss: 0.3080 - learning_rate: 9.0000e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m 217/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11:02\u001b[0m 459ms/step - loss: 1.3082 - species_accuracy: 0.6155 - species_loss: 1.1495 - venom_accuracy: 0.8665 - venom_loss: 0.3175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1653/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 458ms/step - loss: 1.2969 - species_accuracy: 0.6190 - species_loss: 1.1408 - venom_accuracy: 0.8684 - venom_loss: 0.3120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 14:36:05.114404: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - loss: 1.2968 - species_accuracy: 0.6190 - species_loss: 1.1408 - venom_accuracy: 0.8684 - venom_loss: 0.3120\n",
      "Epoch 75: val_loss did not improve from 2.31537\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m852s\u001b[0m 513ms/step - loss: 1.2917 - species_accuracy: 0.6199 - species_loss: 1.1355 - venom_accuracy: 0.8676 - venom_loss: 0.3124 - val_loss: 2.3175 - val_species_accuracy: 0.4858 - val_species_loss: 2.1638 - val_venom_accuracy: 0.8697 - val_venom_loss: 0.3076 - learning_rate: 9.0000e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m 351/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10:04\u001b[0m 461ms/step - loss: 1.2670 - species_accuracy: 0.6111 - species_loss: 1.1120 - venom_accuracy: 0.8657 - venom_loss: 0.3100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 14:40:22.614145: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: extra compressed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1597/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 464ms/step - loss: 1.2853 - species_accuracy: 0.6162 - species_loss: 1.1301 - venom_accuracy: 0.8666 - venom_loss: 0.3104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 558 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1662/1662\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 1.2854 - species_accuracy: 0.6163 - species_loss: 1.1302 - venom_accuracy: 0.8666 - venom_loss: 0.3104"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=n_epochs,\n",
    "    callbacks=[checkpoint_cb, early_stop_cb, reduce_lr_cb],\n",
    "    #class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m.load_weights(\u001b[33m'\u001b[39m\u001b[33mbest_model.keras\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# load best weights back\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_model.keras')  # load best weights back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, species_loss, venom_loss, species_acc, venom_acc = results\n",
    "\n",
    "print(f\"Test species acc: {species_acc*100:0.2f}%\")\n",
    "print(f\"Test venom acc: {venom_acc*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "def example_results_from_dataset(model, ds, species_names, n_examples=5, venom_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Jelenlegi pipeline:\n",
    "    ds yield: (img, (species, venom), (species_w, venom_w))\n",
    "    \"\"\"\n",
    "\n",
    "    # csak (img, labels) kell, slyokat eldobjuk\n",
    "    ds_vis = ds.map(lambda img, labels, *rest: (img, labels))\n",
    "\n",
    "    ds_vis = ds_vis.unbatch().shuffle(1000)\n",
    "    samples = list(ds_vis.take(n_examples))\n",
    "\n",
    "    imgs = [x[0] for x in samples]   # RAW kpek (0255)\n",
    "    lbls = [x[1] for x in samples]\n",
    "\n",
    "    # Modell bemenet elksztse\n",
    "    x_raw = tf.stack([tf.cast(img, tf.float32) for img in imgs], axis=0)\n",
    "    x_for_model = preprocess_input(x_raw)\n",
    "\n",
    "    pred_species_logits, pred_venom_prob = model.predict(x_for_model, verbose=0)\n",
    "\n",
    "    plt.figure(figsize=(3.3 * len(imgs), 3.3))\n",
    "    for i, (img, lbl) in enumerate(zip(imgs, lbls), start=1):\n",
    "        # lbl: (species, venom)\n",
    "        species_lbl, venom_lbl = lbl\n",
    "        true_species = int(species_lbl.numpy())\n",
    "        true_venom   = int(venom_lbl.numpy())\n",
    "\n",
    "        pred_species = int(np.argmax(pred_species_logits[i-1]))\n",
    "        pred_venom   = bool(float(pred_venom_prob[i-1][0]) > venom_threshold)\n",
    "\n",
    "        true_name = species_names[true_species] if 0 <= true_species < len(species_names) else str(true_species)\n",
    "        pred_name = species_names[pred_species] if 0 <= pred_species < len(species_names) else str(pred_species)\n",
    "\n",
    "        plt.subplot(1, len(imgs), i)\n",
    "\n",
    "        img_np = np.clip(img.numpy(), 0, 255).astype(np.uint8)\n",
    "        plt.imshow(img_np)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.title(\n",
    "            f\"True: {true_name} ({'Venom' if true_venom else 'Safe'})\\n\"\n",
    "            f\"Pred: {pred_name} ({'Venom' if pred_venom else 'Safe'})\",\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results_from_dataset(model, val_dataset, species_metadata, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Calculating scoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Function to tell if the species is venomous or not, based on encoded_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_own_metrics = get_scores(model, image_metadata, val_dataset, venom_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Plotting mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_per_class_recall_f1(results):\n",
    "    y_true = results[\"y_species_true\"]\n",
    "    y_pred = results[\"y_species_pred\"]\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    class_ids = sorted([int(c) for c in report.keys() if c.isdigit()])\n",
    "\n",
    "    recalls = [report[str(c)][\"recall\"] for c in class_ids]\n",
    "    f1s     = [report[str(c)][\"f1-score\"] for c in class_ids]\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(class_ids, recalls)\n",
    "    plt.title(\"Per-Class Recall\")\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(class_ids, f1s)\n",
    "    plt.title(\"Per-Class F1-Score\")\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.show()\n",
    "import numpy as np\n",
    "\n",
    "def get_top_confused_pairs(cm, species_names, top_k=20):\n",
    "    \"\"\"\n",
    "    Returns the top most confused class pairs from a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: confusion matrix (shape NxN)\n",
    "        species_names: list mapping class_id -> species name\n",
    "        top_k: how many confused pairs to return\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts with:\n",
    "            true_id, pred_id, true_name, pred_name, count\n",
    "    \"\"\"\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)  # remove correct predictions\n",
    "\n",
    "    confusions = []\n",
    "\n",
    "    # Find all non-zero misclassifications\n",
    "    for true_cls in range(cm_no_diag.shape[0]):\n",
    "        for pred_cls in range(cm_no_diag.shape[1]):\n",
    "            count = cm_no_diag[true_cls, pred_cls]\n",
    "            if count > 0:\n",
    "                confusions.append((true_cls, pred_cls, count))\n",
    "\n",
    "    # Sort by count descending\n",
    "    confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Build output list\n",
    "    results = []\n",
    "    for true_id, pred_id, count in confusions[:top_k]:\n",
    "        results.append({\n",
    "            \"true_id\": true_id,\n",
    "            \"pred_id\": pred_id,\n",
    "            \"true_name\": species_names[true_id],\n",
    "            \"pred_name\": species_names[pred_id],\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(results):\n",
    "    y_true = results[\"y_species_true\"]\n",
    "    y_pred = results[\"y_species_pred\"]\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, cmap='Blues')\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    top_errors = get_top_confused_pairs(cm, species_metadata, top_k=20)\n",
    "\n",
    "    for e in top_errors:\n",
    "        print(f\"True {e['true_id']} ({e['true_name']})  \"\n",
    "            f\"--> Pred {e['pred_id']} ({e['pred_name']})  \"\n",
    "            f\"Count={e['count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_class_recall_f1(results_own_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(results_own_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1792d1-0390-4169-bb15-1380ed158ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
