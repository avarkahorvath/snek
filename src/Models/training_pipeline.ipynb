{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Library imports, setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you change a file, you dont have to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_metadata, visualize_data, make_dataset\n",
    "from model import build_multitask_model\n",
    "from score_metrics import get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7956,
     "status": "ok",
     "timestamp": 1760103668680,
     "user": {
      "displayName": "Avarka",
      "userId": "01376155912533068519"
     },
     "user_tz": -120
    },
    "id": "cb7d91df",
    "outputId": "632fa06d-510c-411c-bd31-1a43c8eb8343"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check tf version\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for device in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f\"Found GPU {device.name}, and set memory growth to True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metadata, species_metadata = load_metadata()\n",
    "NUM_SPECIES = len(species_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in data.py\n",
    "#visualize_data(image_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "d98e31d7"
   },
   "source": [
    "Loading python images from folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_multitask_model\n",
    "from score_metrics import get_scores\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "IMAGE_RESOLUTION=224\n",
    "#IMAGE_RESOLUTION=544\n",
    "\n",
    "\n",
    "NUM_FOLDS = 3\n",
    "\n",
    "from data import make_batches\n",
    "\n",
    "\n",
    "\n",
    "X_paths = image_metadata[\"image_path\"].values\n",
    "y_species = image_metadata[\"encoded_id\"].values\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "\n",
    "best_fold_idx = None\n",
    "best_macro_f1 = -np.inf\n",
    "\n",
    "all_y_species_true = []\n",
    "all_y_species_pred = []\n",
    "all_y_venom_true = []\n",
    "all_y_venom_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_paths, y_species), start=1):\n",
    "    print(f\"\\n===== FOLD {fold_idx}/{NUM_FOLDS} =====\")\n",
    "\n",
    "    train_info = image_metadata.iloc[train_idx].copy()\n",
    "    val_info   = image_metadata.iloc[val_idx].copy()\n",
    "\n",
    "    # --- class weight csak a trainre számolva ---\n",
    "    species_classes = np.unique(train_info[\"encoded_id\"])\n",
    "    species_cw = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=species_classes,\n",
    "        y=train_info[\"encoded_id\"],\n",
    "    )\n",
    "    species_cw_dict = {int(c): w for c, w in zip(species_classes, species_cw)}\n",
    "\n",
    "    species_weight_vec = tf.constant(\n",
    "        [species_cw_dict[i] for i in range(len(species_cw_dict))],\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "\n",
    "    # --- tf.data datasetek ---\n",
    "    train_dataset = make_batches(\n",
    "        train_info,\n",
    "        IMAGE_RESOLUTION,\n",
    "        species_weight_vec=species_weight_vec,\n",
    "    )\n",
    "    val_dataset = make_batches(\n",
    "        val_info,\n",
    "        IMAGE_RESOLUTION,\n",
    "        species_weight_vec=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_paths, y_species), start=1):\n",
    "    print(f\"\\n===== FOLD {fold_idx}/{NUM_FOLDS} =====\")\n",
    "\n",
    "    train_info = image_metadata.iloc[train_idx].copy()\n",
    "    val_info   = image_metadata.iloc[val_idx].copy()\n",
    "\n",
    "    # --- class weight csak a trainre számolva ---\n",
    "    species_classes = np.unique(train_info[\"encoded_id\"])\n",
    "    species_cw = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=species_classes,\n",
    "        y=train_info[\"encoded_id\"],\n",
    "    )\n",
    "    species_cw_dict = {int(c): w for c, w in zip(species_classes, species_cw)}\n",
    "\n",
    "    species_weight_vec = tf.constant(\n",
    "        [species_cw_dict[i] for i in range(len(species_cw_dict))],\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "\n",
    "    # --- tf.data datasetek ---\n",
    "    train_dataset = make_batches(\n",
    "        train_info,\n",
    "        IMAGE_RESOLUTION,\n",
    "        species_weight_vec=species_weight_vec,\n",
    "    )\n",
    "\n",
    "    val_dataset = make_batches(\n",
    "        val_info,\n",
    "        IMAGE_RESOLUTION,\n",
    "        species_weight_vec=None,\n",
    "    )\n",
    "\n",
    "    # --- modell: minden foldban újraépítjük ---\n",
    "    model = build_multitask_model(\n",
    "        num_species=NUM_SPECIES,\n",
    "        image_resolution=IMAGE_RESOLUTION,\n",
    "    )\n",
    "\n",
    "    lr = 5e-4  # EfficientNetB0-hoz alacsony LR\n",
    "\n",
    "    #stage 1: backbone freeze\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss={\n",
    "            \"species\": \"sparse_categorical_crossentropy\", \n",
    "            \"venom\": \"binary_crossentropy\"\n",
    "        },\n",
    "        loss_weights={\"species\": 1.0, \"venom\": 0.5},\n",
    "        metrics={\n",
    "            \"species\": \"accuracy\", \n",
    "            \"venom\": \"accuracy\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # --- callbackek fold-specifikus checkpointtal ---\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        f\"best_model_fold{fold_idx}.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=6,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    reduce_lr_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.3,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    )\n",
    "    n_epochs = 20\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=n_epochs,\n",
    "        callbacks=[checkpoint_cb, early_stop_cb, reduce_lr_cb],\n",
    "    )\n",
    "    #stage 2: unfreeze backbone\n",
    "    model.trainable = True\n",
    "    fine_tune_lr = 1e-5 #ehhez érdemes kisebb LR-t használni\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "        loss={\n",
    "            \"species\": \"sparse_categorical_crossentropy\", \n",
    "            \"venom\": \"binary_crossentropy\"                \n",
    "        },\n",
    "        loss_weights={\"species\": 0.8, \"venom\": 1.2},\n",
    "        metrics={\n",
    "            \"species\": \"accuracy\", \n",
    "            \"venom\": \"accuracy\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ft_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        f\"best_model_fold{fold_idx}_finetuned.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    history_fine = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=40, \n",
    "        callbacks=[ft_checkpoint_cb, ft_early_stop_cb],\n",
    "    )\n",
    "\n",
    "    ft_early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=4,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # --- saját metrikák foldonként, get_scores-szal ---\n",
    "    metrics_fold = get_scores(\n",
    "        model,\n",
    "        image_metadata=image_metadata,\n",
    "        test_dataset=val_dataset,\n",
    "        venom_threshold=0.5,\n",
    "    )\n",
    "\n",
    "    fold_metrics.append(metrics_fold)\n",
    "\n",
    "    if metrics_fold[\"macro_f1\"] > best_macro_f1:\n",
    "        best_macro_f1 = metrics_fold[\"macro_f1\"]\n",
    "        best_fold_idx = fold_idx\n",
    "\n",
    "    \n",
    "    all_y_species_true.append(metrics_fold[\"y_species_true\"])\n",
    "    all_y_species_pred.append(metrics_fold[\"y_species_pred\"])\n",
    "    all_y_venom_true.append(metrics_fold[\"y_venom_true\"])\n",
    "    all_y_venom_pred.append(metrics_fold[\"y_venom_pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- aggregált CV-eredmények (átlag metrikák + összesített predikciók) ---\n",
    "\n",
    "best_model = keras.models.load_model(\n",
    "    f\"best_model_fold{best_fold_idx}.keras\",\n",
    "    custom_objects={\"SoftF1Loss\": SoftF1Loss},\n",
    ")\n",
    "best_model.save(\"final_cv_model.keras\")\n",
    "\n",
    "results_own_metrics = {\n",
    "    \"species_accuracy\": np.mean([m[\"species_accuracy\"] for m in fold_metrics]),\n",
    "    \"macro_f1\": np.mean([m[\"macro_f1\"] for m in fold_metrics]),\n",
    "    \"venom_accuracy\": np.mean([m[\"venom_accuracy\"] for m in fold_metrics]),\n",
    "    \"venom_weighted_species_accuracy\": np.mean(\n",
    "        [m[\"venom_weighted_species_accuracy\"] for m in fold_metrics]\n",
    "    ),\n",
    "    \"y_species_true\": np.concatenate(all_y_species_true, axis=0),\n",
    "    \"y_species_pred\": np.concatenate(all_y_species_pred, axis=0),\n",
    "    \"y_venom_true\": np.concatenate(all_y_venom_true, axis=0),\n",
    "    \"y_venom_pred\": np.concatenate(all_y_venom_pred, axis=0),\n",
    "}\n",
    "\n",
    "print(\"\\n=== Átlagolt keresztvalidációs eredmények ===\")\n",
    "print(f\"Species accuracy (val): {results_own_metrics['species_accuracy']:.4f}\")\n",
    "print(f\"Macro-F1 (species, val): {results_own_metrics['macro_f1']:.4f}\")\n",
    "print(f\"Venom accuracy (val): {results_own_metrics['venom_accuracy']:.4f}\")\n",
    "print(\n",
    "    \"Venom-weighted species accuracy (val): \"\n",
    "    f\"{results_own_metrics['venom_weighted_species_accuracy']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results_from_dataset(model, val_dataset, species_metadata, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Calculating scoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Function to tell if the species is venomous or not, based on encoded_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Plotting mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_per_class_recall_f1(results):\n",
    "    y_true = results[\"y_species_true\"]\n",
    "    y_pred = results[\"y_species_pred\"]\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    class_ids = sorted([int(c) for c in report.keys() if c.isdigit()])\n",
    "\n",
    "    recalls = [report[str(c)][\"recall\"] for c in class_ids]\n",
    "    f1s     = [report[str(c)][\"f1-score\"] for c in class_ids]\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(class_ids, recalls)\n",
    "    plt.title(\"Per-Class Recall\")\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(class_ids, f1s)\n",
    "    plt.title(\"Per-Class F1-Score\")\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.show()\n",
    "import numpy as np\n",
    "\n",
    "def get_top_confused_pairs(cm, species_names, top_k=20):\n",
    "    \"\"\"\n",
    "    Returns the top most confused class pairs from a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: confusion matrix (shape NxN)\n",
    "        species_names: list mapping class_id -> species name\n",
    "        top_k: how many confused pairs to return\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts with:\n",
    "            true_id, pred_id, true_name, pred_name, count\n",
    "    \"\"\"\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)  # remove correct predictions\n",
    "\n",
    "    confusions = []\n",
    "\n",
    "    # Find all non-zero misclassifications\n",
    "    for true_cls in range(cm_no_diag.shape[0]):\n",
    "        for pred_cls in range(cm_no_diag.shape[1]):\n",
    "            count = cm_no_diag[true_cls, pred_cls]\n",
    "            if count > 0:\n",
    "                confusions.append((true_cls, pred_cls, count))\n",
    "\n",
    "    # Sort by count descending\n",
    "    confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Build output list\n",
    "    results = []\n",
    "    for true_id, pred_id, count in confusions[:top_k]:\n",
    "        results.append({\n",
    "            \"true_id\": true_id,\n",
    "            \"pred_id\": pred_id,\n",
    "            \"true_name\": species_names[true_id],\n",
    "            \"pred_name\": species_names[pred_id],\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(results):\n",
    "    y_true = results[\"y_species_true\"]\n",
    "    y_pred = results[\"y_species_pred\"]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, cmap='Blues')\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_class_recall_f1(results_own_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(results_own_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
